# The Google File System

## 摘要

> 原论文在[这里](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)

我们设计并实现了谷歌文件系统（the Google File System, GFS）,一个为大型分布式数据密集型应用设计的可扩展的分布式的文件系统。他运行在不昂贵的商用硬件上，有容错能力，能为大量client提供整体地高性能服务。

与之前设计的分布式文件系统拥有许多相同目标的同时，我们的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。

GFS很成功的满足了我们的存储需求。他作为存储平台在Google内部广泛部署，供需要处理或生成大数据集的在线服务、研发工作使用。当前最大的集群，由数千台机器&&磁盘组成，提供数百TB的存储能力，并提供数百的并发访问能力。

在这篇论文中，我们呈现支持分布式应用的分布式文件系统的接口设计，讨论设计的方方面面，并从微观基准和实际使用中报告测量结果。

## 1. 介绍

我们设计并实现了谷歌文件系统（the Google File System, GFS）,来满足Google数据处理快速增长的需求。与之前设计的分布式文件系统拥有许多相同目标，例如 性能、可扩展性、可靠性、可用性。然而，它的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。

第一，系统构成组件的故障被认定是正常情况 而不是异常情况。这个文件系统由数百甚至数千台不昂贵的商品存储设备组成，并被数量可观的客户端访问。组件的数量和质量基本上确认在有限的时间内一些组件一定无法工作，而且一些组件无法从故障中恢复。我已经遇到过的问题有：应用程序bug、操作系统bug、人员操作错误、磁盘故障、内存故障、链接故障、网络故障、供电故障等。因此，持续监控、错误侦测、容错、自动恢复都必须是系统支持的能力。

第二，传统标准下文件是巨大的。几GB大小的文件很常见。每个文件包含许多应用对象，例如web文档。常常，我们处理由数十亿对象组成的快速增长的数据集时，即使文件系统本可以支持，管理数十亿大约KB大小的文件是很笨重的。因此，必须重新考虑 设计假设和参数，例如 I/O 操作和块大小。

第三，大部分文件修改是追加内容，很少是修改原内容。文件内的随机写更是基本没有。一旦写入，文件仅仅用来读，并且常常是顺序读。许多不同的数据都有这种特性。一些可能构成大型仓库供数据分析程序扫描。一些也许是在线程序持续生成的数据流。一些也许是归档数据。一些也许是一台数据生成要立即或以后被另外机器处理的临时数据。在这种大型文件访问模型下，性能优化的重点是追加写和原子保障，而在客户端缓存数据块完全没用。

第四，协同设计应用程序和文件系统API可以提高我们的灵活性，从而使整个系统受益。例如，放宽GFS的一致性模型来极大的简化文件系统，而不会给应用程序带来沉重的负担。我们引入了原子追加写操作，让多个客户端无需互相通信就可以并发的向一个文件追加内容。详细内容会在论文后续中讨论。

为了不同的目的，已经部署了多套GFS集群。最大的有超过1000个存储结点，超过300TB磁盘容量，并连续被不同机器的数百个client频繁访问。

## 2. 设计概览

### 2.1 基本假设

在位我们需求设计文件系统时，我们一直被既有挑战又有机遇的假设所引导。我们前面提到了一些关键的观察结果，现在更详细的列出我们的列假设。

+ 系统有总会故障的许多廉价的产品零部件组成。他必须持续不断的监控自己，检测、容忍并从组件故障中快速恢复。
+ 系统存储适量的大文件。我们预估预估会有几百万个文件，每个文件通常100MB或者更大。几GB大小的文件是通常情况，需要被有效管理。小文件必须支持，但我们不需要针对他们做优化。
+ 工作负荷主要由两种读组成：大文件顺序读，小文件随机读。在大文件顺序读中，每次操作通常读取几百KB或1MB或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小文件随机读，通常在文件的任意位置读几KB。注重性能的应用程序通常对他们的小文件度进行批处理和排序，以便在文件中稳步前进，而不是来回读取。
+ 工作负荷也会有许多大文件追加数据的顺序写。通常来说，操作大小跟读取差不多。一旦写好，数据基本不再修改。小文件的随机写也支持，但是不需要很高效。
+ 系统必须为多客户端同时写同一文件做良好的设计与有效的实现。我们的文件常常用做 生产-消费 队列，或多路合并。多台机器的几百个客户端，会并发的向一个文件写。具有最小同步开销的原子性，必不可少。文件也许稍后读取，消费者也可以立即读取文件内容。
+ 持续高带宽比低延迟更重要。我们的大多数目标应用程序都重视以高速率处理大量数据，而很少有对单个读取或写入的响应时间做严格要求。

### 2.2 接口

GFS提供了一套熟悉的系统接口，虽然并没有实现类似POSIX的标准API。文件又目录层级组织，并由路径名标识。我们支持了`create`、`delete`、`open`、`close`、`read`和`write`文件的一般接口。

此外，GFS有快照（snapshot）和记录追加（record append）操作。快照可以低成本的给文件或目录创建副本。记录追加可以在保证原子性的同时，支持多个独立客户端并发向同一个文件追加内容。这在实现多路合并 和 支持无需额外锁的多客户端快速追加内容的生产-消费队列。这种类型的文件，在建设分布式系统中非常有用。快照和记录追加会在3.4节和3.3节分别讨论。

### 2.3 架构

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS_1.png)

如图1所示，一个GFS集群，由一个master和多个数据服务器（chunkserver）组成，并被多个客户端访问。每一台设备都是商品Linux机器，运行着用户级服务器进程。只要机器资源允许，在同一台机器上运行chunkserver和客户端是OK的，并且由于运行不稳定的应用程序代码导致的地可靠性是可接受的。

文件被分为固定大小的chunk。每个chunk在被创建时都会被master分配一个全局的不可变的64bit的chunk handle。chunkserver在本地磁盘以Linux文件的形式存储chunk，并读取由 chunk handle和字节范围 标识 的chunk数据。为了可靠性，每个chunk在多台chunkserver做了副本。默认情况下，我们存储3份，用户也可以在不通的文件名字空间的区域指定不同级别的副本。

master维护所有文件系统的元数据。包括 名字空间、访问控制信息、文件到chunk的映射、chunk当前的位置等。它也管理系统纬度的活动，例如chunk租约管理、孤儿chunk的垃圾回收、chunkserver间的chunk迁移。master周期性的以HeartBeat信息的形式跟每个chunkserver通信，分发指令以及收集状态。

链接到应用程序的GFS客户端代码实现文件系统API，并与master和chunkserver通信，以代理应用程序读写数据。客户端与master交互进行元数据操作，但数据通信都直接由chunkserver承担。我们不提供POSIX API，因此，不需要hook Linux vnode层。

client与chunkserver都不缓存文件数据。客户端缓存的收益很小，因为大部分应用顺序的读取大型文件，或者处理的数据集太大无法缓存。并没有通过消除缓存一致性问题来简化客户端和整个系统的复杂度（客户端有缓存元数据）。chunkserver不需要缓存是因为chunk存储在本地磁盘，Linux系统缓存已经为频繁读写的数据在内存做了缓存。

### 2.4 单master

单master的设计极大的简化了我们的设计，也让master能利于全局信息做复杂的chunk放置与备份决策。然而，我们必须在读写时尽可能少卷入master，来防止它成为瓶颈。客户端从来不通过master读写数据。而是会问询他应该跟那个chunkserver联系，在有限的时间内缓存这个信息，接下来一些列操作都直接与chunkserver交互。

让我们参照图1，以读为例解释下这里的交互。首先，客户端将应用程序指定的文件名和字节偏移量转换为文件中的chunk索引。然后，它向master发送一个包含文件名和chunk索引的请求。master返回chunk handle和副本所在的位置。client用文件名和chunk索引为key缓存这些信息。

client然后向其中一个副本发送请求，一般是最近的一个。请求指定chunk handle和该chunk的字节范围。后续直到缓存过期或者文件重开，读同一个chunk都不在需要client-master的交互。实际上，client一般会一次性问询多个chunk信息，master可以立即按要求一次性返回相应的chunk信息。这个额外的信息在几乎没有额外成本的情况下避开了几个未来的client-master交互。

### 2.5 chunk大小

chunk大小是一个关键的设计参数。我们选择一个比一般文件系统块大小大许多的大小，64MB。每个chunk副本都是以Linux文件的形式存储，并只在需要时额外扩展。惰性空间分配（not allocating a resource until it is actually needed）避免了由于内部碎片造成的空间浪费，这可能是此chunk大小的最大缺点。

大chunk有几个重要的优势。第一，减少了client与master交互的需求，因为一个chunk的读写，仅需要在最开始与master交互一次 来获取位置信息。这个所见对我们的负载极其有意义，因为应用程序总是顺序读写大型文件。即使是对于小文件随机读，client也可以恰当的为几TB的处理数据集维护chunk位置信息。第二，因为在一个大chunk上，client常常在这个chunk做多次操作，它可以通过长时间保持与chunkserver的持久TCP 连接来减少网络开销。第三，这减少了master存储的元数据大小。这使得我们可以在内存中保存元数据，这另外带来了其他收益（2.6.1讨论）。

另一方面，大chunk，即使使用惰性空间分配粗策略，也有它的优势。一个小文件仅由几个chunk组成，也有可能只有一个。如果许多客户端一起处理同一个小文件，存储这些chunk的chunkserver也许会成为热点。在实践中，这并没有成为一个主要问题，因为我们的应用基本都是顺序读取用于许多chunk的大型文件。

然而，当GFS第一次用在batch-queue系统中时，热点问题凸显出来：一个可执行文件写到GFS中，其只用一个chunk文件存储，然后同时在数百台机器启动。由于数百的并发请求，存储chunk文件的几个chunkserver就过载了。我们通过 高副本参数 和 交错启动batch-queue系统应用 解决了这个问题。一个可能的长期解决方案是，在这种情况下 允许client从其他client读取数据。

### 2.6 元数据

master存储了三个主要元数据：文件和chunk的名字空间、文件到chunk的映射、每个chunk副本的位置。所有元数据都保存在master的内存中。前两个数据（名字空间 和 file-to-chunk映射）也通过日志的形式写到master本地磁盘的操作日志（operation log）上，并且在远程机器上备份。使用日志的形式，让我们可以简单、可靠地更新master状态，并且在master crash时也不会有不一致的风险。master并不会持久化存储chunk位置信息。master会在启动的时候问询每个chunkserver的chunk信息，以及在有新chunkserver加入集群时。

#### 2.6.1 内存数据结构

因为元数据存储在内存中，master操作执行很快。此外，master周期性地后台扫描他的完整状态是很容易且高效的。周期性扫面用于实现chunk垃圾回收、chunkserver故障时再备份、为平衡负载和磁盘使用空间的chunkserver间的chunk迁移。4.3节和4.4节会讨论这些功能。

这种纯内存方法唯一担心的问题是chunk的数量，因为master的内存大小会因此限制整个系统的容量。在实践中，这并不严重。master为每个64MB的chunk维护小于64字节的元数据。因为大多数文件有多个chunk，所以大多数chunk都是满的，只有文件的最后一个chunk没有填满。类似的，文件的名字空间数据通常小于64字节，因为它使用前缀压缩来紧凑地存储文件名。

如果需要支持更大的文件系统，因 为简单性、可靠性、性能、灵活性的设计 的内存元数据，为master增加内存代价很小。

#### 2.6.2 块位置

master没有持久化的记录哪个chunk的副本在哪些chunkserver。在启动时轮训chunkserver来获取这些信息是很容易的事情。此后，master可以就可以自己保持更新这些数据了，因为它控制着所有chunk的摆放，并且以HearBeat消息的形式监控chunkserver的状态。

起初，我们尝试在master持久化保存chunk位置信息，我们最终这么决策是因为，master在启动时轮训chunkser然后周期性的HeatBeat的方案更简单。这消除了chunkserer加入离开集群、更名、故障、重启等保持master与chunkserver一致的问题，而且这些问题经常发生。

这个设计决策的另一个原因是，chunkserver拥有本地磁盘有什么chunk的最终话语权。尝试在master维护一致性视图是没有意义的，因为chunkserver的错误也许会导致chunk自发消失（例如，磁盘会坏或者不可用）或者一个操作员也许会重命名chunkserver。

#### 2.6.3 操作日志

操作日志包含里关键元数据变更的历史记录。他对GFS来说是重要的。它不仅是元数据的唯一持久记录，而且还充当定义了并发操作顺序的逻辑时间线。文件、chunk和他们的版本（见4.5节），都唯一地永久地被他们被创建的逻辑时间指定。

因为操作日志极其重要，我们必须可靠地存储它，并且直到元数据修改持久化前都不能相应给client。否则，即使块本身幸存下来，我们实际上也会丢失整个文件系统或最近的客户端操作。因此，我们在多台远程机器上做副本，并且只在相应的操作日志记录到本地和远程机器后 才给client回应。master会batch操作日志一次性来减少写磁盘和日志备份对系统整体吞吐量的影响。

master通过重演操作日志来恢复其文件系统状态。为了最小化启动时间，我们必须使日志最小化。每当操作日志增长超过了一定的大小，master会做checkpoint（存档），这样的化，他就可以从本地磁盘获取最近的存档然后只重演此后有限的日志记录。checkpoint是一个紧凑（compact）的B-tree形式，这样可以直接放到内存中，然后无需额外解析就可以用于名字空间查找。这极大地加速了恢复、提高了可用性。

因为创建一个checkpoint需要花点时间，master 的内部状态以这样一种方式构建，即可以在不延迟传入突变的情况下创建新的检查点。master切换到一个新的日志文件，然后一个独立线程创建新的checkpoint。这个checkpoint包含切到新的日志文件前的所有改动。为一个几百万文件的集群创建checkpoint可以在1min以内。完成时，他会被写到本地和远程的磁盘上。

恢复只需要最新完成的checkpoint和之后的日志文件。更老的checkpoint和日志文件都可以删除掉，但是我们为了防止更大的灾难会多存一些。创建checkpoint时的失败并不会引起正确性问题，因为恢复代码检测并跳过未完成的checkpoint。


### 2.7 一致性模型

GFS有一个可以很好支撑我们的高性能分布式应用的relaxed一致性模型，但可以相对简单和有效的实现。我们现在讨论GFS的保障（guarantees），以及他们对应用来说有什么意义。我们也强调GFS是怎么维护这些保障的，但是更多细节在论文后续部分讨论。

#### 2.7.1 GFS的保证

文件名字空间的操作（例如，创建）是原子的。他们仅由master处理：名字空间锁保证了原子性和正确性（4.1节）；master的操作日志定义了这些操作的全局顺序。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS-T1.png ':size=35%')

数据修改后的文件区域的状态，取决于 修改类型、是否成功、是否并发。表1总结了这些结果。如果所有client无论读的是哪个副本获取到的都是同样的数据，那么这个文件区域是一致的（consistene）。如果修改后文件区域是一致的且所有客户端都可以看到改动的内容，那么这个区域是确定的（defined）。如果修改不受并发writer影响地成功后，目标区域是确定的（也是一致的）：所有客户端都能看到修改了什么。并发的成功写让这块区域是不确定的（undifined）但是是一致的：所有client看到同样的数据，但它可能无法反应任何一个修改写入的内容。通常的，他有多个修改的混合片段组成。一个失败的修改另区域不一致（in-consistent），同样也不确定（undefined）：不通的client在不同时间可能会看到不同数据。我们下面描述，我们的应用如何分辨确定区域和不确定区域。应用程序不需要进一步区分不同类型的不确定区域。

数据修改可能是写入（writes）和记录追加（record appends）。写入的含义是写到应用特指的文件偏移的位置。记录追加，即使在并发修改的情况下也能在GFS选定的（3.3节）的偏移处原子的至少追加一次。相对的，记录追加仅仅是在client认为的当前文件结尾处写入数据。偏移量会返回给client，并标识包含该记录的确定区域的开头。另外，GFS可能会在中间插入空白（padding）或者重复记录。它们占据被认为不一致的区域，并且通常与用户数据量相形见绌。

在一些列成功的修改之后，被修改的文件区域保证是确定的，并且包含最后一个修改的写入数据。GFS通过在其所有副本上以相同顺序对块应用修改来实现这一点（3.1节），使用chunk版本号检测任何过期的副本（因为他在chunkserver关闭时错过了修改）（4.5节）。过期的副本永远不会参与修改，也不会给master返回client他的位置。他们是垃圾回收的最早时机。

因为client缓存chunk位置，他们可能在信息刷新前从过期副本读数据。这个窗口被缓存对象的过期时间限制，以及被下一次这个文件open的限制，这会从缓存中清除该文件的所有块信息。此外，我们大部分文件总是仅仅追加写，过期副本常常返回过期的chunk结尾，而不会返回过期的数据。当一个reader重试或者与master联系时，他会立即获取到chunk当前的位置信息。

成功修改很久之后，组件故障当然可能损坏或销毁数据。GFS通过master和chunkserver的正常握手来识别chunksever故障，并通过检验和检测数据损坏（5.2节）。一旦发生问题，数据会尽快从有效的副本重存（4.3节）。只有在几分钟内，GFS能反应过来前，一个chunk的所有副本都损坏（lost）这个chunk才会永久的丢失。即使在这种情况下，数据被确认是不可用，而不是未知损坏：应用会接收到明确错误，而不是损坏的数据。

#### 2.7.2 对应用的影响

GFS应用程序可以通过一些其他目的已经需要的简单技术来适应宽松的一致性模型：依靠追加而不是覆盖、做checkpoint、写自我验证/自我识别的记录。

实际上，我们所有的应用都是追加内容而不是覆盖写。在一个典型的场景中，一个writer从头到尾生成一个文件。写完所有数据后，原子的永久的重命名文件，或周期性的checkpoint已经成功写了多少。checkpoint也包括也许也包括应用级别的校验和。读者只验证和处理到最后一个检查点的文件区域，也就是确定是确定状态的位置。不管是一致性还是并发问题，这种方法都很好解决。追加写远比随机写高效，也更容易从应用故障恢复。checkpoint允许writer递增地重新启动，并防止reader处理从应用程序的角度来看 仍然不完整的 成功写入的文件数据。

在其他典型的使用场景，许多writer为了合并结果并发的追加内容到一个文件，或者作为一个producer-consumer队列。记录追加的追加最少一次（append-at-least-once）的语义保留了每个writer的输出。reader如下处理偶尔的填充数据和重复记录。每个记录都由writer准备了包含校验和的额外信息，这样他的合法性才能被校验。一个reader可以通过校验和识别并忽略额外的填充和记录片段。如果它不能容忍偶然的重复记录（例如，如果他们会触发不幂等的操作），可以使用记录中的唯一ID过滤掉它，应用也通常需要这个东西（例如，web文档）。在Google内，这些record I/O（不包括重复过滤）功能公共库代码的实行在应用中共享，并且适用于其他文件接口实现。这样，相同的记录序列，加上罕见的重复，总是能传递给记录reader。

## 3. 系统交互

我们设计系统来最大限度地减少master在所有操作中的参与。在这种背景下，我们现在描述client、master、chunkserver如何交互来实现数据修改、原子记录追加以及快照。

### 3.1 租约和修改顺序

修改是指一个改动chunk内容或者元数据的操作，例如write和append。每个修改都在chunk的所有副本进行。我们使用租约来保证副本见的一致性修改顺序。master给副本中的一个授予租约，这个被称之为primary。primary组织对这个chunk的一些列修改的顺序。所有副本在应用修改时遵循这个顺序。因此，要定义整体的修改顺序，首先要由master选择租约授权，在一个授权期间，primary安排这个顺序。

租约机制旨在最大限度地减少master的管理开销。一个租约最开始有60s的过期时间。然而，只要chunk还在修改期间，primary可以请求master并通常被应答延长租约。这个请求和应答在master和所有chunkserver的HeartBeat消息中捎带。master也许有时会在过期前撤销租约（例如，当master想要禁用正在重命名的文件的修改时）。即使master与primary丢失通信，当租约过期后，他可以安全的给其他副本授予一个新租约。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS-2.png ':size=35%')

在图2中，我们通过按照这些编号步骤的write的控制流程来说明此过程。

1. client想master问询哪个chunkserver拥有chunk的当前租约以及其他副本的位置。如果没有副本拥有租约，master选一个副本授予租约（未在图中描述）。
2. master回复primary的标识以及其他副本（secondary）的位置。client为了未来的修改缓存这些数据。他只有当primary变得不可达 或者 被primary告知不在用于租约时，才会再与master联系。
3. client给所有副本推送数据。client可以以任意顺序操作。每个chunksever在数据被使用或者过期前，把数据存储在内部LRU buffer cache。通过解藕数据流和控制流，我们可以通过基于网络拓扑调度昂贵的数据流来提高性能，而不用管哪个chunkserver是primary。我们在3.2节更深入讨论。
4. 一旦所有副本被告知接受完数据后，client发送一个write请求给primary。这个请求标识早前push给所有副本的数据。primary为它接收到的所有修改（可能从多个client）分配连续的序列号，这提供了必要的有序化。它以连续序列号的顺序本自己的本地状态执行修改。
5. primary向所有secondary副本转发write请求。每个secondary副本按照master安排的顺序执行修改。
6. 当secondary完成操作后，向primary回复消息。
7. primary回复client。任何副本的错误都会报告给client。在错误的情况下，write也许在primary或者secondary副本的任意子集成功。如果在primary失败，就不会分配序列号然后在secondary继续了。client请求被认为是失败的，修改的区域也处于不一致状态。我们的client对于这种失败修改会重试处理。他在从头重试前，会多次尝试在步骤3～7重试。

如果应用的write很大，或者跨越了chunk边界，GFS client代码把会它分成多个write操作。多个操作会按照上面描述的控制流执行，但是也许会 与其他client的操作交错执行 或 被其他client的操作覆盖。因此，共享的文件区域最终会包含来自不同client的片段，尽管副本将是相同的，因为单个操作在所有副本上以相同的顺序成功完成。这最终导致文件区域consistent但是却是undefined状态。

### 3.2 数据流

我们为了高效地使用网络，解藕了数据流和控制流。控制流从client到primary 然后到所有的secondary，数据以流水线方式沿着精心挑选的chunkserver链线性推送。我们的目标是尽量利用每台机器的网络带宽，避免网络瓶颈和高延迟，最大限度地减少推送所有数据的延迟。

为了充分利用每台机器的网络带宽，数据线性的按chunkserver链推送，而不是按其他拓扑结构（例如，树）的分布式地推送。因此，每台机器的全部出口带宽用于尽可能快地传输数据，而不是在多个接收者之间分配。

为了尽可能避免网络瓶颈和高延迟链接（例如，交换机间的链路），每台机器向他网络拓扑结构中“最近”的还未接收数据的机器推送。假设client要向chunkserver S1～S4发送数据。他向最近的机器发送数据，假如是S1。S1会向S2～S4中离他最近的机器转发数据，假如是S2。类似的，S2向S3、S4中离它最近的机器转发，以此类推。我们的网络拓扑结构是非常简单的，“距离”可以用IP地址准确的评估出来。

最后，我们通过流水线化（pipelining）TCP连接上的数据传输来最小化延迟。一旦chunkserver接受完一些数据，他就立即开始转发。流水线对我们特别有帮助，因为我们使用具有全双工链路的交换网络。立即转发数据并不会影响接受数据的速率。没有网络拥塞的理想情况下，传输B字节给R副本的预估时间是`B/T+RL`，T是网络吞储量，K是两台机器的间传输的延迟。我们的网络连接一般是100Mbps（T），L远小于1ms。因此，1MB理想情况下在80ms左右完成分发。

?> 1 B = 1 byte = 8 bit

### 3.3 原子记录追加

GFS提供了一个原子追加操作，record append。在传统写操作中，client指定数据写入的偏移量。同一文件同一区域的并发写是不可串行化的：这片区域最终可能会有多个client的数据片段。然而在record append中，client仅指定数据。GFS原子的（即，作为一个连续的字节序列）最少一次的把数据追加在文件内一个GFS决策的偏移位置，并返回client这个偏移位置。这与在Unix中以`O_APPEND`打开文件写数据类似，但没有多个writer并发写数据的竞争条件（race condition）问题。

record append在我们的分布式系统中广泛使用，许多不同机器的client并发的向同一个文件追加写。如果是传统模式的写操作，client就会需要复杂且代价昂贵的同步机制，例如分布式锁管理器。在我们工作中，这种文件充当multiple-producer/single-consumer队列 或者 容纳多个不同client的结果的合并。

record append是一种修改，控制流也大体如果3.1节所示，仅仅primary有一点额外的逻辑。client将数据推送到文件最后一个chunk的所有副本，然后将其请求发送到primary。primary检查，如果将数据追加到当前chunk后 会不会导致chunk超过最大大小（64MB）。如果会，primary就在当前chunk填满padding数据，告诉所有secondary也这样做，然后应答客户端说 这个操作将在下一个chunk重试。（record append被限制为最大块大小的四分之一，以将最坏情况的碎片保持在可接受的水平。）如果记录可以满足chunk的大小限制，这也是通常情况，primary把数据追加到他的副本上，然后告诉所有secondary写在primary指定的位置，最后返回成功给client。

如果一个记录在任意副本追加失败，client会重试。因此，同一块的副本可能包含不同的数据，可能包括同一记录的全部或部分副本。GFS不保证所有副本都是字节相同的。他只保证数据是至少一次的原子的单元。这个属性很容易从简单的观察中得出：报告成功的操作，数据必须以相同的偏移量写入某个chunk块的所有副本。而且，在此之后，所有副本一样长，因此 即使其他副本变成primary 未来任何记录都将被分配更高的偏移或新chunk中。依据我们的一致性保障，record append操作成功的区域，被写入的数据是确定的（因此，一致的），尽管这些区域是不一致的（因此，不确定的）。我们的应用可以依照2.7.2节所述处理不一致区域。

### 3.4 快照

快照操作是 在几乎一瞬间 对一个文件或者目录（“source”） 做备份，同时最大限度地减少正在进行的修改的任何中断。我们的用户使用它来快速创建庞大数据集的分支副本（并且经常是这些副本的副本，递归地），或者在提交实验性修改前 给当前状态做checkpoint，或者方便回滚。

像AFS一样，我们利用标准的copy-on-write技术来实现快照。当master收到快照请求，它首先撤销它即将创建快照的文件中块的任何未完成的租约。这确保了对这些块的任何后续写入都需要与 master 进行交互以找到租用持有者。这将使 master 有机会首先创建该块的新副本。

租约被撤销或到期后，master在磁盘记录这个操作（快照）日志。之后通过复制内存中的源文件或目录树的元数据 来执行这个日志记录。新创建的快照文件指向源文件相同的chunk。

在快照操作之后，client首次向chunk（C）写数据时，它向master请求查找当前的租约拥有者是谁。master会注意到chunk（C）的引用者不止一个，它推迟给client的应答，然后挑选一个新chunk（C'）。然后让每个拥有chunk（C）的chunkserver创建一个名字为C'的副本。在chunkserver的本地创建原件的副本，我们确保数据拷贝在本地发生，而不是通过网络（我们的磁盘比网络大概快3倍）。此后，任何chunk的请求处理都没什么差别：master给一个拥有chunk（C'）的chunkserver租约，然后应答给client，然后就可以正常的写chunk，也不用理解这时刚刚复制的chunk。

## 4. master操作

master处理所有的名字空间操作。另外，master管理系统间的chunk备份：副本安置决策、创建新chunk和副本、协调不同的系统维度的操作保证chunk已被完整备份、平衡所有chunkserver的负载、回收未使用的存储。我们现在讨论这些主题。

### 4.1 名字空间管理和锁

许多master的才做都需要花挺长时间：例如，快照操作需要撤销快照覆盖的所有chunk的chunkserver的租约。我们不想在此同时延后处理其他master操作。因此，我们允许多个操作同时进行，在命名空间的区域上使用锁以确保正确的序列化。

不像许多传统文件系统，GFS没有 列出该目录中所有文件的 每个目录的 数据结构。它也不支持同一文件或目录的别名（即 Unix 术语中的硬链接或符号链接）。GFS在逻辑上将其名字空间表示为将完整路径名映射到元数据的查找表。通过前缀压缩，可以在内存中有效地表示该表。名字空间树中的每个节点（文件或者目录的 绝对路径）都有一个关联的read-write锁。

每个master操作在执行前获取一系列锁。典型的，如果操作涉及`/d1/d2/../dn/leaf`，它会获取目录名为的`/d1`，`/d1/d2`……`/d1/d2/.../dn`的read-write锁，一个文件全路径`/d1/d2/.../dn/leaf`的读/写锁。注意取决于操作类型`leaf`可能是文件，也可能是目录。

我们现在说明这种锁定机制如何防止在`/home/user`被快照到`/save/user`时创建文件`/home/user/foo`。快照操作在`/home`和`/save`获取读锁，在`/home/user`和`/save/user`获取写锁。文件创建操作在`/home`和`/home/user`获取读锁，在`/home/user/foo`获取写锁。两个操作可以被合适的序列化，因为他们在尝试获取`/home/user`锁时有冲突。文件创建不用在父目录获取写锁，因为没有“目录“、或者innode样子的数据结构需要被保护。

这种锁定方案的一个很好的特性是它允许在同一目录中并发修改。例如，同一个目录的文件创建可以并发执行：每个操作都获取目录的读锁 和 文件名的写锁。目录名上的读锁足以防止目录被删除、重命名或快照。文件名上的写锁序列化尝试创建具有相同名称的文件两次。

因为名字空间可以有许多结点，读写锁对象被惰性分配并在不使用时删除。而且，以一致的顺序获取锁以防止死锁：按名字空间层级顺序，同级按照字典序。


### 4.2 副本安置（Placement）

GFS集群高度分布在多个级别上。它通常在多个机架有数百台chunkserver。不同机架的两台机器通信，可能会通过一个或更多网络交换机。另外，机架的出入带宽也许比机架内机器的总出入带宽小。多级分发对分发数据的可扩展性、可靠性和可用性提出了独特的挑战。

chunk安置策略有两个原则/目的：最大化数据可靠性和可用性，最大化网络带宽利用率。对于两者来说，仅在机器之间传播副本是不够的，它只能防止磁盘或机器故障并充分利用每台机器的网络带宽。我们必须在机架之间传播副本。这在整个机架损坏或者掉线时（例如，因为网络交换机 或者 电源线故障）仍然能保证chunk的部分副本可用。这也意味着chunk的流量，尤其是读取，可以利用多个机架的总带宽。从另一方面说，写入流量必须流经多个机架，这是我们自愿做出的权衡。

### 4.3 创建、重备份、重平衡

创建chunk副本有三个原因：创建（creation）、重备份（re-replication）、重平衡（rebalancing）。

当master创建一个chunk，它会选择最初的空副本放在哪里。它考虑以下几点。

1. 我们希望将新副本放置在磁盘空间利用率低于平均水平（below-average）的chunkserver上。随着时间过去，这会使chunkserver的的磁盘利用率平均化。
2. 我们希望限制每台chunkserver“最近”创建的数量。虽然创建本身是低成本的，但这基本预示着将会有大量的血流量，因为chunk就是为了写的需求才创建的，在我们append-once-read-many的负载情况，一旦它们被完全写入，它们通常实际上变成只读的。
3. 正如以上的讨论，我们希望在机架间安排副本。

一旦可用副本的数量低于用户指定的目标，master就会重新复制一个chunk。发生这种情况的原因有多种：一个chunkserver变得不可用、它报告它的副本可能已损坏、因为故障它某个磁盘不可用、副本备份目标提高。每个需要重新复制的chunk根据几个因素确定优先级。一个是它离备份目标有多远。举个例子，丢失两个副本的chunk 比 丢失一个副本的chunk 拥有更高优先级。另外，相对于已经删除文件的chunk，我们倾向于复制有效文件（live files）的副本（见4.4节）。最后，为了最小化失败对正在运行的应用的影响，我们提高阻塞client进度的chunk的优先级。

master挑选拥有最好优先级的chunk，通过命令一些chunkserver直接从现存的可用副本拷贝chunk数据来“克隆（clone）”它。新副本的放置目标类似于创建目标：使底盘空间利用率均衡、在任意单个chunkserver上限制活动的克隆的数量、在机架间安排副本。为了防止克隆流量超过client流量，master限制了集群和每个chunkserver的活动克隆操作的数量。此外，每个chunkserver通过限制对源chunkserver的读取请求来限制它在每个克隆操作上花费的带宽量。

最后，master周期性的重平衡副本：它检查当前的副本分布情况，移动副本都磁盘利用率更低、负载更低的机器。同时，在这个过程中，master逐渐填满一个新的chunkserver，而不是立即用新的chunk和随之而来的大量写入流量淹没它。新副本的放置标准与上面讨论的类似。此外，master还必须选择要删除的现有副本。一般来说，它倾向于删除那些可用空间低于平均水平的chunkserver上的chunk，以平衡磁盘空间使用。

### 4.4 垃圾回收

一个文件删除后，GFS不会立即回收可用的物理存储。它只是在文件和chunk级别的常规垃圾收集期间懒惰地这样做。我们发现这种方法使系统更简单、更可靠。

#### 4.4.1 机制

当一个文件被应用删除，就像其他变更一样master立刻记录删除日志（log the deletion）。然后，相较于立即回收资源，文件只是被重编名为一个包含删除时间戳的隐藏的名字。在master常规扫面文件系统名字空间时，它会删除任何已经存在超过三天（可配置）的隐藏文件。在这之前，这个文件用新的特殊的文件名仍然可读，通过重命名为正常名字还可以撤销删除。当隐藏文件从名字空间中删除，它内存中对应的元数据也会删除。这有效地切断了它与所有chunk的链接。

在一次master常规的chunk名字空间扫面中，master识别出成为了孤儿的chunk（即 那些已经无法通过任何文件访问的chunk），然后删除这些chunk的元数据。在一个常规的与master交互的HearBeat消息中，每个chunkserver汇报它拥有的chunk集合，master会应答哪些chunk在他元数据中已经不再维护。chunkserver就可以放心的删除这种chunk的副本。


#### 4.4.2 讨论

尽管分布式垃圾收集在编程语言的上下文中是一个需要复杂解决方案的难题，但在我们的案例中却非常简单。我们可以轻松识别对chunk的所有引用：它们位于由master专门维护的file-to-chunk映射中。我们还可以轻松识别所有chunk副本：它们是每个chunkserver上指定目录下的Linux文件。master不知道的任何此类副本都是“垃圾”。

与急切删除相比，存储回收的垃圾收集方法提供了几个优点。首先，在一个大型的组件故障常见的分布式系统中，这更简单更可靠。chunkserver可能仅在部分chunkserver成功，遗留master不知道的其存在的副本。副本删除消息可能丢失，master不得不在失败发生时记录来重新发送。垃圾回收提供了一个统一的靠得住的方法来清理任何不被认为游泳的副本。第二，它将存储回收合并到master的常规后台活动中，例如名字空间的定期扫描和与chunkserver的握手。因此，它是分批完成的，成本被摊销。而且，只有在master比较空闲的时候才做。master可以更迅速地响应需要及时关注的客户端请求。第三，延迟回收存储提供了 防止意外、不可逆转删除 的安全网。

在我们的经验中，主要缺点是延迟有时会妨碍用户在存储紧张时调整使用情况。重复创建和删除临时文件的应用程序可能无法立即重用存储。 如果再次明确删除已删除的文件，我们会通过加快存储回收来解决这些问题。我们还允许用户对名字空间的不同部分应用不同的复制和回收策略。例如，用户可以指定在某些目录树中的文件的所有副本存储时不需要副本，或者删除的文件立即不可逆的从文件系统状态中删除。

### 4.5 陈旧（Stale）副本检测

如果一个chunkserver故障并错过了对chunk的修改操作，chunk副本可能就变陈旧（Stale）。对每个chunk，master维护一个chunk版本号（version number）来辨别最新chunk和陈旧chunk。

每当master在一个chunk上授予新的租约时，它会增加chunk版本号并通知最新的副本。master和这些副本都会在他们的存储中记录最新版本号。这发生在client被通知之前，即在它能够开始向chunk写之前。如果一个副本当前不可用，它的chunk可能不会被更新。当chunkserver重启或者HeatBeat汇报时，master能检测到chunk是否陈旧。如果master看到一个比他记录的还大的版本号，master假设它在授权时更新自己失败，然后将更大的版本号当成最新版本号。

master在常规垃圾回收中删除陈旧副本。在此之前，在回复client亲故的chunk信息时 它会把陈旧副本当成完全不存在。作为另一个保障，当master通知客户端哪个chunkserver持有一个chunk的租约 或 当它指示一个chunkserver在克隆操作中从另一个chunkserver读取该chunk时，master会包含chunk版本号。client和chunkserver会在执行操作时验证版本号，来确保访问的是最新数据。

## 5. 容错和诊断

我们在设计系统时面临的最大挑战之一是处理频繁的组件故障。组件的质量和数量一起另问题常规化（而非异常情况）：我们不能完全地信任机器，也不能完全信任磁盘。组件故障可能会导致系统不可用，或者更糟糕的数据损坏。我们讨论 我们怎么面对这些挑战，以及，我们在系统内建设的诊断（不可避免的发生的）问题的工具。

### 5.1 高可用

在GFS集群的几百台服务中，在给定的时间内总有一些不可用。我们使用两个简单但有效的策略来保证整体系统的高可用性：快速恢复 和 备份（replication）。

#### 5.1.1 快速恢复

无论master和chunkserver如何终止，它们都被设计为可以在几秒钟内 恢复它们的状态并启动。实际上，我们不区分正常或异常终止；服务通过杀死进程来常规的关闭。client和其他服务器在处理未完成的请求时会遇到小问题，重连到重启的服务，然后重试。6.2.2节会总结观察到的启动时间。

#### 5.1.2 chunk备份（replication）

正如前面的讨论，每个chunk在不同机架的多个chunkserver上备份。用户可以为名字空间的不同部分的文件指定备份级别。默认是3。master根据需要给存在的副本做克隆，来让每个chunk完整的备份，避免因 chunkserver下线、通过检验和检查（见5.2节）检测到的损坏  导致问题。虽然备份机制很好，我们仍然为了日益增长的只读存储的需求 探索跨服务的其他形式的冗余，例如，奇偶校验或者纠删码。我们预计在我们非常松散耦合的系统中实现这些更复杂的冗余方案具有挑战性但易于管理，因为我们的流量主要由追加和读取而不是小的随机写入主导。

#### 5.1.1 master备份（replication）

为了可靠性，master状态需要备份。它的操作日志和checkpoint在多台机器备份。一个状态的修改在执行前，它的日志记录需要已经写入到本地磁盘和所有的master备份。简单起见，一个master仍然负责所有修改以及后台活动，例如内部更改系统的垃圾收集。如果master故障，他几乎可以立即重启。如果它的机器或磁盘故障，GFS外的监控基础设施会在别处使用操作日志副本启动一个新的master进程。client仅使用master的名字（例如，gfs-test），它是一个 在master在其他机器重启时 可改变的DNS别名。

此外，“影子”master（们）在primary master故障时，提供一个访问文件系统的只读能力。他们是影子，不是镜像，他们会稍稍延迟于primary，通常几分之一秒。他们为没有活跃修改动作的文件和不介意获取到稍稍陈旧结果的应用，加强了读可用性。实际上，因为文件内容从chunkserver读取，应用不会观察到陈旧的文件内容。在短时间窗口内，文件元数据（例如目录内容或者访问控制信息）可能时陈旧的。

为了保持自己的知情，影子读取不断增长的操作日志的副本，并将与primary完全相同的更改序列应用于其数据结构。像primary一样，他在启动的时候轮训chunkserver（之后很少）chunk副本的位置，以及频繁交流握手信息，来监控他们的状态。它仅依赖于primary来更新由primary创建和删除副本的决定所导致的副本位置更新。

### 5.2 数据完整性

每个chunkserver使用校验和来检测存储数据的损坏。在GFS集群总是有几百台机器几千个磁盘的情况下，在读写路径上发生磁盘故障导致数据损坏或者丢失是经常的事情。（第7节可以看到一个例子。）我们可以通过其他副本从数据损坏中恢复，但是通过比对不同chunkserver的副本来发现数据顺坏是不切实际的。而且，有歧义的副本也许是合法的：GFS的修改语义，特别的，前面讨论的原子追加记录，不保证副本完全一致。因此，每个chunkserver必须通过维护检验和独立验证它的副本的完整性。

一个chunk由64KB的block组成。每个block有一个对应的32字节的检验和。像其他元数据，校验和也存储在内容中，并用日志与用户数据独立地持久化存储。

对于读来说，chunkserver在相应给请求者（client或者其他chunkserver）之前，验证所有覆盖读取范围的数据block的检验和。因此，chunkserver不会传播损坏数据到其他机器。如果一个记录与记录的校验和不匹配，chunkserver返回错误给请求者，并把这个不匹配回报给master。相应的，请求者会从其他副本读数据，master会从其他副本克隆这个chunk。当一个正确的新副本在且恰当位置，master命令汇报不匹配的chunkserver删除它的副本。

因为一些原因，校验和对读性能有一点影响。因为大多数读扩越多个block，因此我们只需要读取和校验相对少量的额外数据以进行验证。GFS的client代码通过尝试在读时对齐检验和的block边界，来进一步减少这种开销。而且，chunkserver上的校验和查找和比较是在没有任何I/O的情况下完成的，并且校验和计算通常可以与I/O重叠。

校验和计算针对附加到块末尾的写入（而不是覆盖现有数据的写入）进行了大量优化，因为它们在我们的工作负载中占主导地位。我们只是增量更新 最后一个校验和block部分的 校验和，并为 由追加填充的任何全新校验和block 计算新的校验和。即使最后一个部分校验和块已经损坏并且我们现在无法检测到它，新的校验和值也不会与存储的数据匹配，但在下次读取块时会像往常一样检测到损坏。

相比之下，如果写操作覆盖写chunk的已有数据的一段，我们必须 读取并验证 覆盖写范围内从第一个到最后一个block，然后再写，最后计算并记录新的校验和。如果我们不特别的验证  覆盖写范围内从第一个到最后一个block，新的校验和可能隐藏未被覆盖的区域中存在的损坏。

在空闲时间，chunkserver能够扫描并验证不活跃的chunk的内容。这能让我们检测那些基本不会被读的chunk。一旦检测到损坏，master可以创建一个新的未损坏的副本，然后删除损坏的副本。这能防止 master误以为已经有足够的有效的副本但实际数据缺损坏  的情况发生。

### 5.3 诊断工具

广泛而详细的诊断日志在问题隔离、调试和性能分析方面起到了不可估量的作用，同时只产生了最低的成本。没有日志，就很难理解 转瞬即逝的不可重现的 机器间交互。GFS服务生成记录许多有意义的事件的诊断日志（例如chunkserver启动和故障），以及所有RPC请求和应答。这些诊断日志能够在不影响系统正确性的情况下随意删除。然而，我们在空间允许的情况下尽可能的报错这些日志。

RPC日志包括 除了被读取/写入的文件数据内容意外事件的 准确的请求和发送信息。通过匹配请求和应答，以及对照不同机器上的RPC记录，我们能够重建整个交互历史来定位问题。日志也可以为负载测试和性能分析提供线索。

记录日志的性能影响是很小的（相对于收益来说），因为这些日志是顺序的异步的写入。大部分的近期时间也在内存中保存，供持续的在线的监控使用。

## 6. 指标

一些性能数据，不做翻译了。(阅读也先跳过，TODO)

## 7. 经验

在建设和部署GFS的过程中，我们经历了许多不同的问题，一些是执行上的，一些事技术上的。

最初，GFS 被设想为我们生产系统的后端文件系统。随着时间的过去，用途演变为包括研究和开发任务。它开始时几乎不支持权限和配额等内容，但现在包括这些的基本形式。虽然生产系统受到良好的纪律和控制，但用户有时却不然。 需要更多的基础设施来防止用户相互干扰。

我们一些较大问题是磁盘和Linux相关问题。我们的许多磁盘都向Linux驱动程序声称它们支持一系列IDE协议版本，但实际上只对较新的版本做出可靠响应。由于协议版本非常相似，这些驱动器大部分都可以工作，但偶尔不匹配会导致驱动器和内核对驱动器的状态产生分歧。由于内核中的问题，这会悄悄地破坏数据。这个问题促使我们使用校验和来检测数据损坏，同时我们修改了内核来处理这些协议不匹配。

早些时候，由于 fsync() 的开销，我们在 Linux 2.2 内核上遇到了一些问题。它的成本与文件的大小而不是修改部分的大小成正比。这对于我们的大型操作日志来说是一个问题，尤其是在我们实施检查点之前。我们通过使用同步写入解决了这个问题，最终迁移到了Linux 2.4。

另一个Linux问题是单个reader-writer锁，地址空间中的任何线程在从磁盘调入（reader锁）或在`mmap()`调用中修改地址空间（writer锁）时都必须持有该锁。我们在轻负载下看到系统中的瞬时超时，并努力寻找资源瓶颈或零星的硬件故障。最终，我们发现这个单一的锁阻止了主网络线程将新数据映射到内存，而磁盘线程正在对先前映射的数据进行分页。由于我们主要受网络接口而不是内存复制带宽的限制，因此我们通过以额外副本为代价将`mmap()`替换为`pread()`来解决此问题。

尽管偶尔会出现问题，但Linux代码的可用性一次又一次地帮助我们探索和理解系统行为。在适当的时候，我们会改进内核并与开源社区共享更改。

## 8. 相关工作

与其他的对比，暂不翻译。（阅读也先跳过，TODO）

## 9. 总结

Google 文件系统展示了在商品硬件上支持大规模数据处理工作负载所必需的品质。 虽然一些设计决策特定于我们独特的环境，但许多可能适用于类似规模和成本意识的数据处理任务。

我们首先根据我们当前和预期的应用程序工作负载和技术环境重新检查传统文件系统假设。 我们的观察导致了设计空间中完全不同的点。 我们将组件故障视为常态而不是例外，针对大多数附加（可能并发）然后读取（通常顺序）的大文件进行优化，并扩展和放松标准文件系统接口以改进整个系统。

我们的系统通过持续监控、复制关键数据以及快速自动恢复来提供容错能力。chunk复制允许我们容忍chunkserver故障。这些故障的频率激发了一种新颖的在线修复机制，该机制定期透明地修复损坏并尽快补偿丢失的副本。此外，我们使用校验和来检测磁盘或IDE子系统级别的数据损坏，鉴于系统中的磁盘数量，这变得非常普遍。

我们的设计为执行各种任务的许多并发reader和writer提供了很高的总吞吐量。我们通过将 master的文件系统控制 与 直接在chunkserver和client之间传递的数据传输分离来实现这一点。通过chunk大小和chunk租约，将在数据修改中权限委托给primary副本，从而最大限度地减少了master参与的常见操作。这使得 一个不会成为瓶颈的简单、集中的master 成为可能。我们相信我们网络堆栈的改进 将提高 当前对单个client的写入的 吞吐量限制。

GFS已成功满足我们的存储需求，并在Google内部广泛用作研发和生产数据处理的存储平台。它是一个重要的工具，使我们能够在整个网络的规模上继续创新和解决问题。


