# MapReduce

Simplified Data Processing on Large Clusters.

## 1. 介绍

原论文介绍就不翻译了，本文是从[这里](https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf)阅读原文时，为了更好的记忆消化，而做的个人翻译。

## 2. 编程模型

计算过程以一组`key/values`对为输入，然后产出一组`key/values`对输出。MapReduce库的用户使用两个函数表达整个计算过程：`Map`和`Reduce`。

+ `Map`以一个`key/values`对为输入，然后生成一组`key/values`对*中间数据*。MapReduce库会统一组织所有`Map`生成的中间数据，把相同`key`（say as `I`）的所有`values`，传递给`Reduce`函数。
+ `Reduce`以一个`key`（say as `I`）以及相关的一组中间数据为入参。把这组数据合并为更小的集合。通常每次`Reduce`调用只返回0或1个值。中间数据通过一个迭代器传递给`Reduce`。这使得我们可以处理放不下内存的数据量。

### 2.1 举例

以统计超大文档集合中各个单词数量的问题为例。用户编写的代码如下（伪码）：

```
map(String key, String value):
    // key:   文档名
    // value: 文档内容
    for each word w in value:
        EmitIntermedite(w, "1");

reduce(String key, Iterator values):
    // key:    一个单词
    // values：一系列的counts
    int result = 0;
    for each v in values:
        result += ParseInt(v);
    Emit(AsString(result));
```

`map`函数对每一个单词emit一个出现次数的关联数据（该例子中就是1）。`reduce`函数把这个单词所有emit的数据，加和在一起。

### 2.2 参数类型

虽然伪代码例子中的入参出参的类型是String，但是从逻辑上看，两个函数的参数类型具有关联性：

```
map(k1,v1)    --------→ list(k2,v2)
reduce(k2,list(v2)) --→ list(v2)
```

也就是说，入参的类型跟出参不是同一领域，中间数据跟出参是同一领域。

### 2.3 其他例子

+ 分布式grep操作：`map`函数判断如果匹配了grep模式则emit这行，`reduce`就是一个等价转发函数，把中间数据转发为输出。
+ 计算URL的访问频率：`map`函数处理网页访问日志，并emit`<URL,1>`。`reduce`函数把同一URL的关联的数据加合并输出`<URL,total count>`。
+ 反转网站引用图：`map`函数处理所有引用关系并emit每一个`<target,source>`。`reduce`函数归并入中间数据为`<target,list(source)>`。
+ 倒排索引：`map`函数解析文档并emit一系列的`<word,document ID>`。`reduce`函数接受一个word的一些列中间数据，按`document ID`排序并输出`<word,list(document ID)>`。这一系列的输出组成一个简单的倒排索引。增加这个计算过程来追踪单词出现的位置是一件很容易的事情。
+ 分布式排序：`map`函数从每一个记录里面抽取key，并emit一组`<key,record>`。`reduce`函数也是等价转发。这个计算过程依赖于 4.1节 描述的分区设施，以及 4.2节 描述的排序特性。

## 3. 实现

可以有很多实现方式，来满足MapReduce定义的接口。正确的方式取决于使用环境。例如，一种实现对小内存机器适用，另一种实现对大型[NUMA](https://zh.wikipedia.org/wiki/%E9%9D%9E%E5%9D%87%E5%8C%80%E8%AE%BF%E5%AD%98%E6%A8%A1%E5%9E%8B)多处理适用，还有一种对更大集合的网络互联的机器集群适用，等等。

这一节我们描述一个 以在Google广泛使用的计算环境（通过因特网组成的大型商品PC集群） 实现。在我们环境中：

1. 机器是典型的x86双核处理器，运行Linux操作系统，并有2～4G内存。
2. 使用商品网络硬件--通常每台机器有100Mb/s或者1Gb/s能力，但是整体的对分带宽要少一些。
3. 一个集群有成百上千台机器组成，也因此单机故障时有发生。
4. 存储能力由不算昂贵的IDE磁盘承担（独立挂载在每台机器上）。这些磁盘由内部开发的一个分布式文件系统管理。基于可靠的硬件，这个文件系统通过`replication`的机制保障可用性和可靠性。
5. 用户想调度系统提交工作。每个工作包含一组任务，被调度器分配到一个集群的一组可用机器上。

### 3.1 执行概览

`Map`调用是多机分布式的，输入数据会自动的分割为M份。输入数据就可以被不同机器并行处理。`Reduce`调用也是多机分布式的，使用分片函数（例如：`hash(key) mod R`）按中间数据的key把其分割为R份。分区数字R，以及分片函数由用户自己定义。

图1展示了一个MapReduce执行的整体过程。当用户调用了MapReduce函数，以下动作顺序执行：

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/MapReduce_1.png)

1. 用户程序的MapReduce库最开始把输入文件分成M片，每片通常16MB～64MB（可用通过参数设置）。随后在集群机器上启动该程序的副本。
2. 有一个程序副本是特殊特--master副本。其他的是worker，master给worker分配任务。一共有M个map任务和R个reduce任务，每一个任务分配给一个空闲的worker。
3. 分配map任务的worker，读取对应输入文件的内容。从输入数据中解析`key/vales`对，并传递给用户定义的Map函数。Map函数生成的中间数据缓存在内存中。
4. 缓存的键值对周期性的写到本地磁盘，通过分片函数分割为R份。这些缓存数据和磁盘数据的位置信息会传递给master，master负责分发这些信息给reduce worker。
5. 当一个这些信息通知到reduce worker后，reduce worker使用RPC从map worker的本地磁盘读取这些数据。当一个reduce worker拿到它负责的所有中间数据后，会根据中间数据的key排序，以达到相同key的数据可以一起处理的目的。通常来说，许多不通的key会放到一个reduce任务，所以这个排序是必须的。如果中间数据过大，会使用外部排序。
6. reduce woker遍历所有排序过的中间数据，对每一个中间数据key，把这个key和所有相关数据传递给用户定义的Reduce函数。Reduce函数的输出会追加到这个reduce任务的最终输出文件内。
7. 当所有map任务和reduce任务完成。master回调用户程序。这时，MapReduce调用用户程序返回到用户代码中。

全部执行完毕后，MapReduce程序生成R个输出文件。通常，用户不需要合并这些输出文件，他们可以把文件传递给另一个MapReduce调用，或者给一个能处理这种输出数据的分布式程序。

### 3.2 master数据结构

master持续维护几个数据结构。对于每个map任务和reduce任务，维护一个状态（idle，in-progress 或者 completed）。（为每一个非idle任务）维护worker机器的ID。

master是map任务生成的中间数据到reduce任务的中间人。因此，对于每个完成的map任务，master存储其R份中间数据的位置和大小。map任务完成时，会发送给master以供其更新这份数据。这个数据在渐进式的推送给有in-progress reduce任务的worker。

### 3.3 容错性

因为MapReduce库是为使用成百上千机器处理非常大量数据而设计的，这个库必须可以优雅的处理机器故障。

#### worker故障

master会周期性的ping每个worker。如果没有在一定时间收到worker的应答，master就会标记它为失败。任何map任务被worker执行完毕，都会被重置回最初的idle状态，因此就有资格被重新安排到其他worker执行。类似的，执行任何map任务、reduce任务的worker故障了，这个任务可以重置为idle状态，然后就可以被重新安排。

已完成的map任务被重新执行的情况是，中间数据存储在执行map任务的worker上，一旦worker故障，中间数据就无法访问了。已完成的reduce任务不用重新执行，因为他们的输出是存储在全局文件系统中。

当一个map任务被worker A执行，随后被worker B重新执行（因为A故障了），所有执行reduce任务的worker会被通知。任务还没有A读取完数据的reduce任务，会从B读取数据。

MapReduce可以从大量worker故障中快捷回复。例如，在一个MapReduce执行中，网络维护导致80台机器一同几分钟不可访问。MapRedece的master简单的重新执行这些不可访问worker的任务，继续推进，最终完成MapReduce操作。

#### master故障

master周期性的给master数据结构做checkpoint是很容易的，如果master故障，可以从上一个checkpoint的状态恢复master。然后，考虑到master只有一个，它不太可能失败。因此，我们当前的实现中，如果master故障了，就中断MapReduce计算。用户调用端可以知道这种情况，也可以重新发起MapReduce操作。

#### 出现故障时的语义

如果用户提供的map/reduce操作在固定的输入下是确定的，我们的分布式执行结果，跟无失败的顺序执行的整体程序执行结果，一样。

我们依赖map/reduce任务输出的原子提交来实现这个特性。每个in-progress任务把它的输出写到私有的临时文件中。一个reduce任务生成一个这样的文件，一个map任务生成R个这样的文件。当一个map任务完成，worker发送包含R个临时文件名的信息给master。如果一个master从一个已经完成map任务的wroker收到完成信息，它会忽略这个信息。否则，会把这个R个文件的名字记录到master数据结构中。

当一个reduce任务完成，reduce worker原子地把临时文件重命名为最终的输出文件。如果有其他机器执行着相同的任务，相同的重命名操作会执行多次。我们依赖文件系统基础提供的重命名原子能力，保障最终文件系统中包含这个任务的输出文件。

我们大部分的map/reduce操作都是确定性的，等同于单体顺序执行结果，是的程序员可以非常简单的为程序行为做出解释。如果map/reduce操作是不确定性的，我们提供较弱的但是合理的语义。在非确定操作的情况下，特定任务$R_1$的输出可能等同于顺序执行的非确定性程序。然后，另一个reduce任务$R_2$可能跟另外一个顺序执行的非确定性程序一致。

考虑map任务$M$和reduce任务$R_1$和$R_2$。用$e(R_i)$表示任务$R_i$的结果。当$e(R_1)$数据源是某种M结果，$e(R_2)$数据源是另外一种M结果，就会发生若语义。

### 3.4 本地化

在我们的环境中，网络带宽是一种稀缺资源。通过充分利用特性：输入数据（GFS管理）存储在计算集群的机器的本次磁盘，我们可以节省网络带宽。GFS把文件分割成64MB的块，并在多台机器储存副本（一般是3副本）。MapReduce的master会考虑到输入文件的位置信息，尝试把map任务安排在有对应数据副本的机器。如果不能，就就近安排（例如，同一子网的另一台机器）。我们MapReduce操作的大部分worker集群，大部分的数据数据都是读本地，消耗极少的网络带宽。

### 3.5 任务粒度

正如上面描述的，我们吧map阶段分为M片，把reduce阶段分为R片。理想情况下，M和R应该远大于worker机器的数量。让每个worker处理许多不通的任务，有利于提升动态负载平衡，也有助于加快worker故障的恢复速度：任务重试可以在其他所有worker上展开。

在我们的环境中，M和R有实际的最大阈值，因为master需要处理$O(M+R)$个调度决策和在内存中保存$O(M*R)$个状态。(然而这个复杂度的常量比较小：$O(M*R)$片状态 由 每个map/reduce任务对大概1字节数据 组成)

此外，R总会被用户限制，因为每个reduce任务会产出一个独立的输出文件。在实践中，我们倾向于选择合适的M使得每个独立的map任务的输入数据大小在16MB～64MB之间（使得上面描述的本地化优化能有最大性能），我们一般令R为机器数量的几倍。我们总是倾向于使用如下配置完成MapReduce计算：M=20w，R=5000，2000台worker机器。


### 3.6 支援任务

一个的导致MapReduce操作总体耗时增加的常见原因是"落后者"：在完成最后几个map/reduce计算中护花费异常多时间的机器。有多种原因都会导致落后者出现。例如，一个机器磁盘故障需要花费更多磁盘访问以修复错误可能会导致磁盘度速度从30MB降到1MB。集群调度模块也许会分配任务到已分配任务机器上，导致发生CPU、内存、磁盘、网络带宽的竞争，最终导致MapReduce代码执行效率降低。近期，我们发生了一个导致处理器cache失效的机器初始化bug，该机器上的任务因此降速上百倍。

我们有一个通用的方法来减轻落后者的问题。当MapReduce操作接近完成时，master给还在in-progress的任务进行支援调度。无论原任务完成还是支援任务完成，这个任务都会被标记为已完成。我们已经对这一机制进行过优化，通常来说，MapReduce操作花费的计算资源只会增加几个百分点。并且这一机制对完成大型MapReduce操作的时间有显著减少。举个例子，如果没有这个支援机制，5.3节提到的排序程序会多花费44%时间。

## 4. 精细优化（Refinements）

虽然，对大部分需求来说，简单编写Map和Reduce函数所提供的基本功能，已经足够了。但是，我们还有一些非常有用的扩展。

### 4.1 分片函数

MapReduce的用户可以指定 reduce任务/输出文件 的个数（$R$）。数据依据中间数据的key通过分片函数来进行分片。默认的分片函数是hash（例如，$hash(key) mod R$）。这可以使得数据和负载相当平衡。然而，在一些例子中，使用其他的分片函数更合适。例如，输出数据以URL为key，我们想同一个域名的数据输出在同一个文件。为了达到这种目的，MapReduce库的用户需要自己提供分片函数。分片函数例如，$hash(Hostname(urlkey)) mod R$，这样可以让相同域名的数据最终输出在同一个文件。

### 4.2 有序性保障

我们保证在一个分片里，按key的升序处理中间数据。这个有序性保障使得用户可以方便的生成一个有序的输出数据，如果有助于实现输出数据按key的随机访问，输出文件的有序性也会很方便用户的后续处理。

### 4.3 合成函数

某些情况下，map任务会产生非常多重复key的中间数据，同时用户的Reduce函数是满足交换律、结合律的。一个很好的例如子是，2.1节提到的（英文）单词计数，因为单词的频率分布满足`Zipf`分布，每个map任务都会产生成百上千的`<the,1>`记录。所有这些记录都会通过网络传递到reduce任务，然后被Reduce函数加和生成一个数字。我们允许用户定一个Combiner函数，在数据发送出去前，先本地做合并处理。

Combiner函数就在执行map任务的机器本地执行。通常来说，Combiner函数的代码跟Reduce函数一样。唯一的区别是MapReduce如何处理他们：Reduce函数输出的数据会写到最终输出文件里，Combiner函数输出的数据会写到（发送给reduce任务）的临时数据文件里。

局部合并对某些MapReduce操作有显著提速的效果。附录A列举了一些例子。

### 4.4 输入和输出类型

MapReduce库支持几种不通的读取输入数据的格式。例如，"text"格式的输入数据，每一行会被当成一个`key/value`对：key是偏移量，value是这行的内容。另一种常见的支持格式存储按key排序的`key/value`对序列（这句没理解，但是没大影响）。大多用户使用预定义的输入数据类型就足够了，但也可以自行实现reader函数来支持新的输入数据类型。

reader不一定从文件读取数据，可以从数据库、内存数据结构等获取数据。

类似的，我们提供了几种输出数据的格式，用户也可以自定义输出数据的格式。

### 4.5 副作用

在某些情况下，MapReduce的用户发现从他们的map和/或reduce操作生成辅助文件作为附加输出很方便。我们依赖应用的写能力来确保副产物的原子性和幂等性。一般来说，应用写数据到临时文件，当执行完毕后自动重命名文件。

对于一个任务产生多个文件的情况，我们并没有支持两阶段提交级别的原子性。因此，生成具有跨文件一致性要求的多个输出文件的任务应该是确定性的。这种限制在实践中从来不是问题。

### 4.6 跳过坏记录

有时候，用户代码会有导致Map函数或Reduce函数crash的确定性bug（仅遇到某些记录发生）。通常的解决方案是修复这个bug，但是某些时候不可行；例如，bug在一个源码不可看的第三方的库里。另外，有时候忽略一些记录也是可接受的，例如超大数据的统计分析。我们提供了一个可选模式，在这种模式下，MapReduce库可以检测导致程序crash的特定记录，忽略这些记录，来继续推进执行。

每个worker进程都有一个可以catch段违规和总线错误的信号处理handler。在调用用户定义的Map函数或Reduce函数前，MapReduce库先把一些参数存到全局变量中。如果用户代码产生了这种错误信号，信号处理handler在挂掉前给master发送一个包含这些参数的UDP包。当master看到同一个记录导致多于一次失败时，下次重新执行任务时会指定跳过该记录。

### 4.7 本地执行

Map函数或Reduce函数的debug非常棘手，因为计算过程是在分布式系统中进行，几千台机器的任务由master动态决策。为了方便debug、分析、小范围测试，我们开发了一个MapReduce的替代实现，可以在本地顺序的执行MapReduce操作的所有工作。给用户提供了指定执行特定map任务的能力。用户通过特定参数启动程序后，就可以方便的使用任何debug或测试工具（例如，gdb）。

### 4.8 状态信息

master上有一个内部HTTP服务，给人提供一系列的状态页面。状态页面展示计算的进展，例如多少任务已经完成、多少在进行中、输入数据的字节数、中间数据的字节数、输出数据的字节数、计算进度等。这个页面也有指向标准错误和每个任务产生的标准输出文件链接。用户可以利用这些数据预估计算过程还需要多久完成，是否需要增加更多资源。这个页面也可以用来弄清楚为什么计算进展比预期慢。

另外，顶级的状态页面，展示哪些worker失败了，以及失败的map/reduce任务。当你尝试定位用户代码的bug时，这是非常有用的。

### 4.9 计数器

MapReduce库提供了计数器工具，可以给不同事件的发生计数。例如，用户想统计已处理的单词的总数 或 已索引的德文文档的数量，等等。

用户代码创建一个名为counter的对象，然后就可以在Map和/或Reduce函数里操作它（增加），例如：

```
Counter * uppercase;
uppercase = getCounter("uppercase")

map(string name, String contents):
    for each word w in contents:
        if (IsCapitalized(w)):
            uppercase->Increment();
        EmitIntermediate(w, "1");
```

每个worker机器的计数器的值会定期的传给master（ping应答时捎带返回）。当map/reduce任务成功结束后，master对计数器的值进行聚合。整个MapReduce操作结束后，会返回给用户代码。当前的计数器的值也会被展示在master的状态页面，人们可以了解在线计算的进度。master在聚合计数器的值时，会消除重复任务造成的重复计数。（重复计数可能发生在：支援任务 或者 失败任务的重新执行）

MapReduce库会原生维护一些计数器的值，例如已处理的输出`key/value`对的数量，生成的`key/value`对的数量。

计数器工具对MapReduce操作的行为的合理性检查非常有用。例如，在某些MapReduce操作中，用户代码希望确保处理的输出`key/value`对的数量等于生成的`key/value`对的数量，或者，处理的德文文档的比例在全部处理文档的一定范围内。

## 5. 性能

这一节中，我们度量两个运行在大集群的MapReduce计算过程的性能。一个是从TB数据中搜索特定pattern，另一个是给TB数据排序。

这是MapReduce用户可能执行的代表性程序--一类程序把数据换一种展示顺序，另一类程序从大数据集中抽取小量感兴趣的数据。

## 5.1 集群配置

所有的程序都在由大约1800台机器组成的集群上运行。每台机器拥有2个 2GHz Inter Xeon处理器（HyperThreading enabled）、4GB内存，2个160GB IDE磁盘，以及1Gb网络。这些机器由2层的树状交换网络组织，根部带宽大约100～200Gbps。所有的机器用同一套网络托管设备，因此任意两台机器的RTT小雨1ms。

总共4GB内存，大约1～1.5GB为运行在集群上的其他任务预留。程序在周末下午运行，此时CPU、磁盘、网络基本是空闲状态。

## 5.2 Grep

grep程序扫描$10^10$100字节的记录，搜索一个相对少见的3字符pattern（在92337个记录中出现）。输出数据按大约64MB分片（M=15000），整个输出放在同一个文件（R
=1）。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/MapReduce-2.png)

图2展示计算过程中的进展。Y轴表示输入数据被读取的速率。随着任务被安排到不同机器，速率不断攀升。峰值在1764个worker安排任务时，超过30GB/s。当map任务结束后，速率开始下降，80s后到0。整个计算过程从开始到计数大约150s。包括大约1分钟的启动。头部启动是由于要传程序到各个机器上，以及打开1000个输入文件时与GFS交互的延迟，以及为了本地化优化收集信息。

## 5.3 Sort

sort程序给$10^10$100字节的记录排序（大约1TB数据）。该程序以 TeraSort 基准为模型。

排序程序的代码少于50行。3行Map函数代码从一行数据中解析10字节的key，并emit key和原始数据。我们使用一个内置的原地转发Reduce操作。这个函数原封不动的把传入的中间数据输出。最终的输出结果写到2-way复制的GFS文件。

像前面一样，输入数据被以64MB分片（M=15000）。排序后的结果输出到4000个文件（R=4000）。分片函数使用key的原始字节把它分到R片中的一个。

在这个基准测试中，我们的分片函数天然知道key的分布。而在一般的排序程序中，我们会增加一个预先的MapReduce操作，收集key的样本并利用key样本的分布计算最终排序的分割点。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/MapReduce-3.png)

图3(a)展示了排序程序的正常执行过程。最上面的图展示了输入数据的读取速率。峰值在13GB/s，200s内所有map任务完成，速率很快就掉0。注意输入数据的速率小于grep程序。这是一用排序map任务在本身处理时间仅仅相当于grep程序的一半，写临时数据到磁盘时占用的I/O时间较多。相应的，grep程序输出的中间数据相对于sort程序可以忽略不计。

左中图展示了map任务的临时数据传送给reduce任务的速率。排序从第一个map任务完成后开始。图中第一个峰是一批大约1700个reduce任务执行（整个MapReduce任务分配了1700台机器，每个机器同时只执行一个reduce任务）。大约在300s的时候，第一批reduce任务完成，然后就启动了剩余的reduce任务。所有任务大约在600s的时候完成。

左下图展示了reduce任务输出最终结果文件的速率。第一批reduce任务完成到开始写输出文件中间有延迟，因为机器正忙与给中间数据排序。写大概以速率2～4GB/s持续了一段时间。所有的写在850s的时候完成。包括头部启动，整个计算过程话费了891秒。这跟TeraSort基准的最佳记录1057秒相近。

一些值得注意的点：输入数据的速率比排序速率、输出速率高，这是本地化优化的原因--大部分数据从本地读取而不是用相对受限的网络带宽。排序速率大于输出速率，因为输出阶段写数据有2分副本（因为可靠性和可用性的原因，底层文件系统对输出数据做了2副本）。如果底层文件系统使用纠删码（而不是副本）网络带宽会省很多。

### 5.4 支援任务的影响

图3(b)中，我们展示了关闭支援任务的执行过程。除了有一个长尾的写（那时写操作很少），整个过程跟图3(a)很相似。960秒以后，大部分任务完成（剩余5个长尾）。然而，最后的落后者一直到300秒后才完成。整个计算过程花费1283秒，占用时间增长了44%。

### 5.5 机器故障

图3(c)中，我们展示了故意kill掉1746worker中的200个的排序程序的计算过程。集群底层的调度器很快重启了机器上的worker进程（因为只有worker被kill掉，机器功能仍然正常）。

worker挂掉在图中显示为负的输入数据速率，因为之前一些已经完成的map工作消失（对应的map任务被kill）然后需要重做。kill掉map任务在相当短的时间内的重新执行。整个计算过程在933秒内结束（包括头部启动时间）。仅仅比正常情况多了5%时间。

## 6. 经验

我们在2003年2月写了第一版MapReduce库，然后在2003年8月做了增强，包括本地化优化、负载动态平衡等。从那以后，我们对MapReduce库对我们处理的各种问题的广泛适用感到惊喜。它已经在Goodle内被不同领域广泛使用，包括：

+ 大型的机器学习问题
+ Google新闻和Froogle产品的集群问题
+ 提取数据用于流行搜索的报告（例如，Google Zeitgeist）
+ 为新的实验/产品提取网页属性（例如，为本地化搜索从网站大语料中提取地理位置信息）
+ 大型图计算

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/MapReduce-4.png)

图4展示了在我们主代码库中MapReduce程序的显著增长情况，从2003年的0到2004年9月的900。MapReduce的成功是它使在半小时内写一个简单程序在成百上千机器上有效的运行成为可能。极大的加速了开发和原型周期。此外，它使得没有分布式/并行系统经验的人，也可以很方便的利用大量的资源。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/MapReduce_T1.png)

在每个任务的最后，MapReduce库记录分析了任务的计算过程用的资源。在表1中，我们展示了2004年8月Google运行MapReduce任务的部分数据。

### 6.1 大型索引

## 7. 相关工作

## 8. 结论
