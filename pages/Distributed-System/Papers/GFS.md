# The Google File System

## 摘要

我们设计并实现了谷歌文件系统（the Google File System, GFS）,一个为大型分布式数据密集型应用设计的可扩展的分布式的文件系统。他运行在不昂贵的商用硬件上，有容错能力，能为大量client提供整体地高性能服务。

与之前设计的分布式文件系统拥有许多相同目标的同时，我们的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。

GFS很成功的满足了我们的存储需求。他作为存储平台在Google内部广泛部署，供需要处理或生成大数据集的在线服务、研发工作使用。当前最大的集群，由数千台机器&&磁盘组成，提供数百TB的存储能力，并提供数百的并发访问能力。

在这篇论文中，我们呈现支持分布式应用的分布式文件系统的接口设计，讨论设计的方方面面，并从微观基准和实际使用中报告测量结果。

## 1. 介绍

我们设计并实现了谷歌文件系统（the Google File System, GFS）,来满足Google数据处理快速增长的需求。与之前设计的分布式文件系统拥有许多相同目标，例如 性能、可扩展性、可靠性、可用性。然而，它的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。

第一，系统构成组件的故障被认定是正常情况 而不是异常情况。这个文件系统由数百甚至数千台不昂贵的商品存储设备组成，并被数量可观的客户端访问。组件的数量和质量基本上确认在有限的时间内一些组件一定无法工作，而且一些组件无法从故障中恢复。我已经遇到过的问题有：应用程序bug、操作系统bug、人员操作错误、磁盘故障、内存故障、链接故障、网络故障、供电故障等。因此，持续监控、错误侦测、容错、自动恢复都必须是系统支持的能力。

第二，传统标准下文件是巨大的。几GB大小的文件很常见。每个文件包含许多应用对象，例如web文档。常常，我们处理由数十亿对象组成的快速增长的数据集时，即使文件系统本可以支持，管理数十亿大约KB大小的文件是很笨重的。因此，必须重新考虑 设计假设和参数，例如 I/O 操作和块大小。

第三，大部分文件修改是追加内容，很少是修改原内容。文件内的随机写更是基本没有。一旦写入，文件仅仅用来读，并且常常是顺序读。许多不同的数据都有这种特性。一些可能构成大型仓库供数据分析程序扫描。一些也许是在线程序持续生成的数据流。一些也许是归档数据。一些也许是一台数据生成要立即或以后被另外机器处理的临时数据。在这种大型文件访问模型下，性能优化的重点是追加写和原子保障，而在客户端缓存数据块完全没用。

第四，协同设计应用程序和文件系统API可以提高我们的灵活性，从而使整个系统受益。例如，放宽GFS的一致性模型来极大的简化文件系统，而不会给应用程序带来沉重的负担。我们引入了原子追加写操作，让多个客户端无需互相通信就可以并发的向一个文件追加内容。详细内容会在论文后续中讨论。

为了不同的目的，已经部署了多套GFS集群。最大的有超过1000个存储结点，超过300TB磁盘容量，并连续被不同机器的数百个client频繁访问。

## 2. 设计概览

### 2.1 基本假设

在位我们需求设计文件系统时，我们一直被既有挑战又有机遇的假设所引导。我们前面提到了一些关键的观察结果，现在更详细的列出我们的列假设。

+ 系统有总会故障的许多廉价的产品零部件组成。他必须持续不断的监控自己，检测、容忍并从组件故障中快速恢复。
+ 系统存储适量的大文件。我们预估预估会有几百万个文件，每个文件通常100MB或者更大。几GB大小的文件是通常情况，需要被有效管理。小文件必须支持，但我们不需要针对他们做优化。
+ 工作负荷主要由两种读组成：大文件顺序读，小文件随机读。在大文件顺序读中，每次操作通常读取几百KB或1MB或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小文件随机读，通常在文件的任意位置读几KB。注重性能的应用程序通常对他们的小文件度进行批处理和排序，以便在文件中稳步前进，而不是来回读取。
+ 工作负荷也会有许多大文件追加数据的顺序写。通常来说，操作大小跟读取差不多。一旦写好，数据基本不再修改。小文件的随机写也支持，但是不需要很高效。
+ 系统必须为多客户端同时写同一文件做良好的设计与有效的实现。我们的文件常常用做 生产-消费 队列，或多路合并。多台机器的几百个客户端，会并发的向一个文件写。具有最小同步开销的原子性，必不可少。文件也许稍后读取，消费者也可以立即读取文件内容。
+ 持续高带宽比低延迟更重要。我们的大多数目标应用程序都重视以高速率处理大量数据，而很少有对单个读取或写入的响应时间做严格要求。

### 2.2 接口

GFS提供了一套熟悉的系统接口，虽然并没有实现类似POSIX的标准API。文件又目录层级组织，并由路径名标识。我们支持了`create`、`delete`、`open`、`close`、`read`和`write`文件的一般接口。

此外，GFS有快照（snapshot）和记录追加（record append）操作。快照可以低成本的给文件或目录创建副本。记录追加可以在保证原子性的同时，支持多个独立客户端并发向同一个文件追加内容。这在实现多路合并 和 支持无需额外锁的多客户端快速追加内容的生产-消费队列。这种类型的文件，在建设分布式系统中非常有用。快照和记录追加会在3.4节和3.3节分别讨论。

### 2.3 架构

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS_1.png)

如图1所示，一个GFS集群，由一个master和多个数据服务器（chunkserver）组成，并被多个客户端访问。每一台设备都是商品Linux机器，运行着用户级服务器进程。只要机器资源允许，在同一台机器上运行chunkserver和客户端是OK的，并且由于运行不稳定的应用程序代码导致的地可靠性是可接受的。

文件被分为固定大小的chunk。每个chunk在被创建时都会被master分配一个全局的不可变的64bit的chunk handle。chunkserver在本地磁盘以Linux文件的形式存储chunk，并读取由 chunk handle和字节范围 标识 的chunk数据。为了可靠性，每个chunk在多台chunkserver做了副本。默认情况下，我们存储3份，用户也可以在不通的文件名字空间的区域指定不同级别的副本。

master维护所有文件系统的元数据。包括 名字空间、访问控制信息、文件到chunk的映射、chunk当前的位置等。它也管理系统纬度的活动，例如chunk租约管理、孤儿chunk的垃圾回收、chunkserver间的chunk迁移。master周期性的以HeartBeat信息的形式跟每个chunkserver通信，分发指令以及收集状态。

链接到应用程序的GFS客户端代码实现文件系统API，并与master和chunkserver通信，以代理应用程序读写数据。客户端与master交互进行元数据操作，但数据通信都直接由chunkserver承担。我们不提供POSIX API，因此，不需要hook Linux vnode层。

client与chunkserver都不缓存文件数据。客户端缓存的收益很小，因为大部分应用顺序的读取大型文件，或者处理的数据集太大无法缓存。并没有通过消除缓存一致性问题来简化客户端和整个系统的复杂度（客户端有缓存元数据）。chunkserver不需要缓存是因为chunk存储在本地磁盘，Linux系统缓存已经为频繁读写的数据在内存做了缓存。

### 2.4 单master

单master的设计极大的简化了我们的设计，也让master能利于全局信息做复杂的chunk放置与备份决策。然而，我们必须在读写时尽可能少卷入master，来防止它成为瓶颈。客户端从来不通过master读写数据。而是会问询他应该跟那个chunkserver联系，在有限的时间内缓存这个信息，接下来一些列操作都直接与chunkserver交互。

让我们参照图1，以读为例解释下这里的交互。首先，客户端将应用程序指定的文件名和字节偏移量转换为文件中的chunk索引。然后，它向master发送一个包含文件名和chunk索引的请求。master返回chunk handle和副本所在的位置。client用文件名和chunk索引为key缓存这些信息。

client然后向其中一个副本发送请求，一般是最近的一个。请求指定chunk handle和该chunk的字节范围。后续直到缓存过期或者文件重开，读同一个chunk都不在需要client-master的交互。实际上，client一般会一次性问询多个chunk信息，master可以立即按要求一次性返回相应的chunk信息。这个额外的信息在几乎没有额外成本的情况下避开了几个未来的client-master交互。

### 2.5 chunk大小

chunk大小是一个关键的设计参数。我们选择一个比一般文件系统块大小大许多的大小，64MB。每个chunk副本都是以Linux文件的形式存储，并只在需要时额外扩展。惰性空间分配（not allocating a resource until it is actually needed）避免了由于内部碎片造成的空间浪费，这可能是此chunk大小的最大缺点。

大chunk有几个重要的优势。第一，减少了client与master交互的需求，因为一个chunk的读写，仅需要在最开始与master交互一次 来获取位置信息。这个所见对我们的负载极其有意义，因为应用程序总是顺序读写大型文件。即使是对于小文件随机读，client也可以恰当的为几TB的处理数据集维护chunk位置信息。第二，因为在一个大chunk上，client常常在这个chunk做多次操作，它可以通过长时间保持与chunkserver的持久TCP 连接来减少网络开销。第三，这减少了master存储的元数据大小。这使得我们可以在内存中保存元数据，这另外带来了其他收益（2.6.1讨论）。

另一方面，大chunk，即使使用惰性空间分配粗策略，也有它的优势。一个小文件仅由几个chunk组成，也有可能只有一个。如果许多客户端一起处理同一个小文件，存储这些chunk的chunkserver也许会成为热点。在实践中，这并没有成为一个主要问题，因为我们的应用基本都是顺序读取用于许多chunk的大型文件。

然而，当GFS第一次用在batch-queue系统中时，热点问题凸显出来：一个可执行文件写到GFS中，其只用一个chunk文件存储，然后同时在数百台机器启动。由于数百的并发请求，存储chunk文件的几个chunkserver就过载了。我们通过 高副本参数 和 交错启动batch-queue系统应用 解决了这个问题。一个可能的长期解决方案是，在这种情况下 允许client从其他client读取数据。

### 2.6 元数据

master存储了三个主要元数据：文件和chunk的名字空间、文件到chunk的映射、每个chunk副本的位置。所有元数据都保存在master的内存中。前两个数据（名字空间 和 file-to-chunk映射）也通过日志的形式写到master本地磁盘的操作日志（operation log）上，并且在远程机器上备份。使用日志的形式，让我们可以简单、可靠地更新master状态，并且在master crash时也不会有不一致的风险。master并不会持久化存储chunk位置信息。master会在启动的时候问询每个chunkserver的chunk信息，以及在有新chunkserver加入集群时。

#### 2.6.1 内存数据结构

因为元数据存储在内存中，master操作执行很快。此外，master周期性地后台扫描他的完整状态是很容易且高效的。周期性扫面用于实现chunk垃圾回收、chunkserver故障时再备份、为平衡负载和磁盘使用空间的chunkserver间的chunk迁移。4.3节和4.4节会讨论这些功能。

这种纯内存方法唯一担心的问题是chunk的数量，因为master的内存大小会因此限制整个系统的容量。在实践中，这并不严重。master为每个64MB的chunk维护小于64字节的元数据。因为大多数文件有多个chunk，所以大多数chunk都是满的，只有文件的最后一个chunk没有填满。类似的，文件的名字空间数据通常小于64字节，因为它使用前缀压缩来紧凑地存储文件名。

如果需要支持更大的文件系统，因 为简单性、可靠性、性能、灵活性的设计 的内存元数据，为master增加内存代价很小。

#### 2.6.2 块位置

master没有持久化的记录哪个chunk的副本在哪些chunkserver。在启动时轮训chunkserver来获取这些信息是很容易的事情。此后，master可以就可以自己保持更新这些数据了，因为它控制着所有chunk的摆放，并且以HearBeat消息的形式监控chunkserver的状态。

起初，我们尝试在master持久化保存chunk位置信息，我们最终这么决策是因为，master在启动时轮训chunkser然后周期性的HeatBeat的方案更简单。这消除了chunkserer加入离开集群、更名、故障、重启等保持master与chunkserver一致的问题，而且这些问题经常发生。

这个设计决策的另一个原因是，chunkserver拥有本地磁盘有什么chunk的最终话语权。尝试在master维护一致性视图是没有意义的，因为chunkserver的错误也许会导致chunk自发消失（例如，磁盘会坏或者不可用）或者一个操作员也许会重命名chunkserver。

#### 2.6.3 操作日志

操作日志包含里关键元数据变更的历史记录。他对GFS来说是重要的。它不仅是元数据的唯一持久记录，而且还充当定义了并发操作顺序的逻辑时间线。文件、chunk和他们的版本（见4.5节），都唯一地永久地被他们被创建的逻辑时间指定。

因为操作日志极其重要，我们必须可靠地存储它，并且直到元数据修改持久化前都不能相应给client。否则，即使块本身幸存下来，我们实际上也会丢失整个文件系统或最近的客户端操作。因此，我们在多台远程机器上做副本，并且只在相应的操作日志记录到本地和远程机器后 才给client回应。master会batch操作日志一次性来减少写磁盘和日志备份对系统整体吞吐量的影响。

master通过重演操作日志来恢复其文件系统状态。为了最小化启动时间，我们必须使日志最小化。每当操作日志增长超过了一定的大小，master会做checkpoint（存档），这样的化，他就可以从本地磁盘获取最近的存档然后只重演此后有限的日志记录。checkpoint是一个紧凑（compact）的B-tree形式，这样可以直接放到内存中，然后无需额外解析就可以用于名字空间查找。这极大地加速了恢复、提高了可用性。

因为创建一个checkpoint需要花点时间，master 的内部状态以这样一种方式构建，即可以在不延迟传入突变的情况下创建新的检查点。master切换到一个新的日志文件，然后一个独立线程创建新的checkpoint。这个checkpoint包含切到新的日志文件前的所有改动。为一个几百万文件的集群创建checkpoint可以在1min以内。完成时，他会被写到本地和远程的磁盘上。

恢复只需要最新完成的checkpoint和之后的日志文件。更老的checkpoint和日志文件都可以删除掉，但是我们为了防止更大的灾难会多存一些。创建checkpoint时的失败并不会引起正确性问题，因为恢复代码检测并跳过未完成的checkpoint。


### 2.7 一致性模型

GFS有一个可以很好支撑我们的高性能分布式应用的relaxed一致性模型，但可以相对简单和有效的实现。我们现在讨论GFS的保障（guarantees），以及他们对应用来说有什么意义。我们也强调GFS是怎么维护这些保障的，但是更多细节在论文后续部分讨论。

#### 2.7.1 GFS的保证

文件名字空间的操作（例如，创建）是原子的。他们仅由master处理：名字空间锁保证了原子性和正确性（4.1节）；master的操作日志定义了这些操作的全局顺序。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS-T1.png ':size=35%')

数据修改后的文件区域的状态，取决于 修改类型、是否成功、是否并发。表1总结了这些结果。如果所有client无论读的是哪个副本获取到的都是同样的数据，那么这个文件区域是一致的（consistene）。如果修改后文件区域是一致的且所有客户端都可以看到改动的内容，那么这个区域是确定的（defined）。如果修改不受并发writer影响地成功后，目标区域是确定的（也是一致的）：所有客户端都能看到修改了什么。并发的成功写让这块区域是不确定的（undifined）但是是一致的：所有client看到同样的数据，但它可能无法反应任何一个修改写入的内容。通常的，他有多个修改的混合片段组成。一个失败的修改另区域不一致（in-consistent），同样也不确定（undefined）：不通的client在不同时间可能会看到不同数据。我们下面描述，我们的应用如何分辨确定区域和不确定区域。应用程序不需要进一步区分不同类型的不确定区域。

数据修改可能是写入（writes）和记录追加（record appends）。写入的含义是写到应用特指的文件偏移的位置。记录追加，即使在并发修改的情况下也能在GFS选定的（3.3节）的偏移处原子的至少追加一次。相对的，记录追加仅仅是在client认为的当前文件结尾处写入数据。偏移量会返回给client，并标识包含该记录的确定区域的开头。另外，GFS可能会在中间插入空白（padding）或者重复记录。它们占据被认为不一致的区域，并且通常与用户数据量相形见绌。

在一些列成功的修改之后，被修改的文件区域保证是确定的，并且包含最后一个修改的写入数据。GFS通过在其所有副本上以相同顺序对块应用修改来实现这一点（3.1节），使用chunk版本号检测任何过期的副本（因为他在chunkserver关闭时错过了修改）（4.5节）。过期的副本永远不会参与修改，也不会给master返回client他的位置。他们是垃圾回收的最早时机。

因为client缓存chunk位置，他们可能在信息刷新前从过期副本读数据。这个窗口被缓存对象的过期时间限制，以及被下一次这个文件open的限制，这会从缓存中清除该文件的所有块信息。此外，我们大部分文件总是仅仅追加写，过期副本常常返回过期的chunk结尾，而不会返回过期的数据。当一个reader重试或者与master联系时，他会立即获取到chunk当前的位置信息。

成功修改很久之后，组件故障当然可能损坏或销毁数据。GFS通过master和chunkserver的正常握手来识别chunksever故障，并通过检验和检测数据损坏（5.2节）。一旦发生问题，数据会尽快从有效的副本重存（4.3节）。只有在几分钟内，GFS能反应过来前，一个chunk的所有副本都损坏（lost）这个chunk才会永久的丢失。即使在这种情况下，数据被确认是不可用，而不是未知损坏：应用会接收到明确错误，而不是损坏的数据。

#### 2.7.2 对应用的影响

GFS应用程序可以通过一些其他目的已经需要的简单技术来适应宽松的一致性模型：依靠追加而不是覆盖、做checkpoint、写自我验证/自我识别的记录。

实际上，我们所有的应用都是追加内容而不是覆盖写。在一个典型的场景中，一个writer从头到尾生成一个文件。写完所有数据后，原子的永久的重命名文件，或周期性的checkpoint已经成功写了多少。checkpoint也包括也许也包括应用级别的校验和。读者只验证和处理到最后一个检查点的文件区域，也就是确定是确定状态的位置。不管是一致性还是并发问题，这种方法都很好解决。追加写远比随机写高效，也更容易从应用故障恢复。checkpoint允许writer递增地重新启动，并防止reader处理从应用程序的角度来看 仍然不完整的 成功写入的文件数据。

在其他典型的使用场景，许多writer为了合并结果并发的追加内容到一个文件，或者作为一个producer-consumer队列。记录追加的追加最少一次（append-at-least-once）的语义保留了每个writer的输出。reader如下处理偶尔的填充数据和重复记录。每个记录都由writer准备了包含校验和的额外信息，这样他的合法性才能被校验。一个reader可以通过校验和识别并忽略额外的填充和记录片段。如果它不能容忍偶然的重复记录（例如，如果他们会触发不幂等的操作），可以使用记录中的唯一ID过滤掉它，应用也通常需要这个东西（例如，web文档）。在Google内，这些record I/O（不包括重复过滤）功能公共库代码的实行在应用中共享，并且适用于其他文件接口实现。这样，相同的记录序列，加上罕见的重复，总是能传递给记录reader。

## 3. 系统交互

我们设计系统来最大限度地减少master在所有操作中的参与。在这种背景下，我们现在描述client、master、chunkserver如何交互来实现数据修改、原子记录追加以及快照。

### 3.1 租约和修改顺序

修改是指一个改动chunk内容或者元数据的操作，例如write和append。每个修改都在chunk的所有副本进行。我们使用租约来保证副本见的一致性修改顺序。master给副本中的一个授予租约，这个被称之为primary。primary组织对这个chunk的一些列修改的顺序。所有副本在应用修改时遵循这个顺序。因此，要定义整体的修改顺序，首先要由master选择租约授权，在一个授权期间，primary安排这个顺序。

租约机制旨在最大限度地减少master的管理开销。一个租约最开始有60s的过期时间。然而，只要chunk还在修改期间，primary可以请求master并通常被应答延长租约。这个请求和应答在master和所有chunkserver的HeartBeat消息中捎带。master也许有时会在过期前撤销租约（例如，当master想要禁用正在重命名的文件的修改时）。即使master与primary丢失通信，当租约过期后，他可以安全的给其他副本授予一个新租约。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS-2.png ':size=35%')

在图2中，我们通过按照这些编号步骤的write的控制流程来说明此过程。

1. client想master问询哪个chunkserver拥有chunk的当前租约以及其他副本的位置。如果没有副本拥有租约，master选一个副本授予租约（未在图中描述）。
2. master回复primary的标识以及其他副本（secondary）的位置。client为了未来的修改缓存这些数据。他只有当primary变得不可达 或者 被primary告知不在用于租约时，才会再与master联系。
3. client给所有副本推送数据。client可以以任意顺序操作。每个chunksever在数据被使用或者过期前，把数据存储在内部LRU buffer cache。通过解藕数据流和控制流，我们可以通过基于网络拓扑调度昂贵的数据流来提高性能，而不用管哪个chunkserver是primary。我们在3.2节更深入讨论。
4. 一旦所有副本被告知接受完数据后，client发送一个write请求给primary。这个请求标识早前push给所有副本的数据。primary为它接收到的所有修改（可能从多个client）分配连续的序列号，这提供了必要的有序化。它以连续序列号的顺序本自己的本地状态执行修改。
5. primary向所有secondary副本转发write请求。每个secondary副本按照master安排的顺序执行修改。
6. 当secondary完成操作后，向primary回复消息。
7. primary回复client。任何副本的错误都会报告给client。在错误的情况下，write也许在primary或者secondary副本的任意子集成功。如果在primary失败，就不会分配序列号然后在secondary继续了。client请求被认为是失败的，修改的区域也处于不一致状态。我们的client对于这种失败修改会重试处理。他在从头重试前，会多次尝试在步骤3～7重试。

如果应用的write很大，或者跨越了chunk边界，GFS client代码把会它分成多个write操作。多个操作会按照上面描述的控制流执行，但是也许会 与其他client的操作交错执行 或 被其他client的操作覆盖。因此，共享的文件区域最终会包含来自不同client的片段，尽管副本将是相同的，因为单个操作在所有副本上以相同的顺序成功完成。这最终导致文件区域consistent但是却是undefined状态。

### 3.2 数据流

我们为了高效地使用网络，解藕了数据流和控制流。控制流从client到primary 然后到所有的secondary，数据以流水线方式沿着精心挑选的chunkserver链线性推送。我们的目标是尽量利用每台机器的网络带宽，避免网络瓶颈和高延迟

### 3.3 原子记录追加

## 4. master的操作

## 5. 容错和诊断

## 6. 指标

## 7. 经验

## 8. 相关工作

## 9. 总结
