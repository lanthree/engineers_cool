# The Google File System

## 摘要

我们设计并实现了谷歌文件系统（the Google File System, GFS）,一个为大型分布式数据密集型应用设计的可扩展的分布式的文件系统。他运行在不昂贵的商用硬件上，有容错能力，能为大量client提供整体地高性能服务。

与之前设计的分布式文件系统拥有许多相同目标的同时，我们的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。

GFS很成功的满足了我们的存储需求。他作为存储平台在Google内部广泛部署，供需要处理或生成大数据集的在线服务、研发工作使用。当前最大的集群，由数千台机器&&磁盘组成，提供数百TB的存储能力，并提供数百的并发访问能力。

在这篇论文中，我们呈现支持分布式应用的分布式文件系统的接口设计，讨论设计的方方面面，并从微观基准和实际使用中报告测量结果。

## 1. 介绍

我们设计并实现了谷歌文件系统（the Google File System, GFS）,来满足Google数据处理快速增长的需求。与之前设计的分布式文件系统拥有许多相同目标，例如 性能、可扩展性、可靠性、可用性。然而，它的设计由 应用的负载 和 技术环境的现状与可预见的未来 驱动，这也导致与早先的文件系统的基本假设由显著背离。这使得我们复查历史决策并探讨不同的基本设计要点。

第一，系统构成组件的故障被认定是正常情况 而不是异常情况。这个文件系统由数百甚至数千台不昂贵的商品存储设备组成，并被数量可观的客户端访问。组件的数量和质量基本上确认在有限的时间内一些组件一定无法工作，而且一些组件无法从故障中恢复。我已经遇到过的问题有：应用程序bug、操作系统bug、人员操作错误、磁盘故障、内存故障、链接故障、网络故障、供电故障等。因此，持续监控、错误侦测、容错、自动恢复都必须是系统支持的能力。

第二，传统标准下文件是巨大的。几GB大小的文件很常见。每个文件包含许多应用对象，例如web文档。常常，我们处理由数十亿对象组成的快速增长的数据集时，即使文件系统本可以支持，管理数十亿大约KB大小的文件是很笨重的。因此，必须重新考虑 设计假设和参数，例如 I/O 操作和块大小。

第三，大部分文件修改是追加内容，很少是修改原内容。文件内的随机写更是基本没有。一旦写入，文件仅仅用来读，并且常常是顺序读。许多不同的数据都有这种特性。一些可能构成大型仓库供数据分析程序扫描。一些也许是在线程序持续生成的数据流。一些也许是归档数据。一些也许是一台数据生成要立即或以后被另外机器处理的临时数据。在这种大型文件访问模型下，性能优化的重点是追加写和原子保障，而在客户端缓存数据块完全没用。

第四，协同设计应用程序和文件系统API可以提高我们的灵活性，从而使整个系统受益。例如，放宽GFS的一致性模型来极大的简化文件系统，而不会给应用程序带来沉重的负担。我们引入了原子追加写操作，让多个客户端无需互相通信就可以并发的向一个文件追加内容。详细内容会在论文后续中讨论。

为了不同的目的，已经部署了多套GFS集群。最大的有超过1000个存储结点，超过300TB磁盘容量，并连续被不同机器的数百个client频繁访问。

## 2. 设计概览

### 2.1 基本假设

在位我们需求设计文件系统时，我们一直被既有挑战又有机遇的假设所引导。我们前面提到了一些关键的观察结果，现在更详细的列出我们的列假设。

+ 系统有总会故障的许多廉价的产品零部件组成。他必须持续不断的监控自己，检测、容忍并从组件故障中快速恢复。
+ 系统存储适量的大文件。我们预估预估会有几百万个文件，每个文件通常100MB或者更大。几GB大小的文件是通常情况，需要被有效管理。小文件必须支持，但我们不需要针对他们做优化。
+ 工作负荷主要由两种读组成：大文件顺序读，小文件随机读。在大文件顺序读中，每次操作通常读取几百KB或1MB或更多。来自同一客户端的连续操作通常会读取文件的连续区域。小文件随机读，通常在文件的任意位置读几KB。注重性能的应用程序通常对他们的小文件度进行批处理和排序，以便在文件中稳步前进，而不是来回读取。
+ 工作负荷也会有许多大文件追加数据的顺序写。通常来说，操作大小跟读取差不多。一旦写好，数据基本不再修改。小文件的随机写也支持，但是不需要很高效。
+ 系统必须为多客户端同时写同一文件做良好的设计与有效的实现。我们的文件常常用做 生产-消费 队列，或多路合并。多台机器的几百个客户端，会并发的向一个文件写。具有最小同步开销的原子性，必不可少。文件也许稍后读取，消费者也可以立即读取文件内容。
+ 持续高带宽比低延迟更重要。我们的大多数目标应用程序都重视以高速率处理大量数据，而很少有对单个读取或写入的响应时间做严格要求。

### 2.2 接口

GFS提供了一套熟悉的系统接口，虽然并没有实现类似POSIX的标准API。文件又目录层级组织，并由路径名标识。我们支持了`create`、`delete`、`open`、`close`、`read`和`write`文件的一般接口。

此外，GFS有快照（snapshot）和记录追加（record append）操作。快照可以低成本的给文件或目录创建副本。记录追加可以在保证原子性的同时，支持多个独立客户端并发向同一个文件追加内容。这在实现多路合并 和 支持无需额外锁的多客户端快速追加内容的生产-消费队列。这种类型的文件，在建设分布式系统中非常有用。快照和记录追加会在3.4节和3.3节分别讨论。

### 2.3 架构

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS_1.png)

如图1所示，一个GFS集群，由一个master和多个数据服务器（chunkserver）组成，并被多个客户端访问。每一台设备都是商品Linux机器，运行着用户级服务器进程。只要机器资源允许，在同一台机器上运行chunkserver和客户端是OK的，并且由于运行不稳定的应用程序代码导致的地可靠性是可接受的。

文件被分为固定大小的chunk。每个chunk在被创建时都会被master分配一个全局的不可变的64bit的chunk handle。chunkserver在本地磁盘以Linux文件的形式存储chunk，并读取由 chunk handle和字节范围 标识 的chunk数据。为了可靠性，每个chunk在多台chunkserver做了副本。默认情况下，我们存储3份，用户也可以在不通的文件名字空间的区域指定不同级别的副本。

master维护所有文件系统的元数据。包括 名字空间、访问控制信息、文件到chunk的映射、chunk当前的位置等。它也管理系统纬度的活动，例如chunk租约管理、孤儿chunk的垃圾回收、chunkserver间的chunk迁移。master周期性的以HeartBeat信息的形式跟每个chunkserver通信，分发指令以及收集状态。

链接到应用程序的GFS客户端代码实现文件系统API，并与master和chunkserver通信，以代理应用程序读写数据。客户端与master交互进行元数据操作，但数据通信都直接由chunkserver承担。我们不提供POSIX API，因此，不需要hook Linux vnode层。

client与chunkserver都不缓存文件数据。客户端缓存的收益很小，因为大部分应用顺序的读取大型文件，或者处理的数据集太大无法缓存。并没有通过消除缓存一致性问题来简化客户端和整个系统的复杂度（客户端有缓存元数据）。chunkserver不需要缓存是因为chunk存储在本地磁盘，Linux系统缓存已经为频繁读写的数据在内存做了缓存。

### 2.4 单master

单master的设计极大的简化了我们的设计，也让master能利于全局信息做复杂的chunk放置与备份决策。然而，我们必须在读写时尽可能少卷入master，来防止它成为瓶颈。客户端从来不通过master读写数据。而是会问询他应该跟那个chunkserver联系，在有限的时间内缓存这个信息，接下来一些列操作都直接与chunkserver交互。

让我们参照图1，以读为例解释下这里的交互。首先，客户端将应用程序指定的文件名和字节偏移量转换为文件中的chunk索引。然后，它向master发送一个包含文件名和chunk索引的请求。master返回chunk handle和副本所在的位置。client用文件名和chunk索引为key缓存这些信息。

client然后向其中一个副本发送请求，一般是最近的一个。请求指定chunk handle和该chunk的字节范围。后续直到缓存过期或者文件重开，读同一个chunk都不在需要client-master的交互。实际上，client一般会一次性问询多个chunk信息，master可以立即按要求一次性返回相应的chunk信息。这个额外的信息在几乎没有额外成本的情况下避开了几个未来的client-master交互。

### 2.5 chunk大小

chunk大小是一个关键的设计参数。我们选择一个比一般文件系统块大小大许多的大小，64MB。每个chunk副本都是以Linux文件的形式存储，并只在需要时额外扩展。惰性空间分配（not allocating a resource until it is actually needed）避免了由于内部碎片造成的空间浪费，这可能是此chunk大小的最大缺点。

大chunk有几个重要的优势。第一，减少了client与master交互的需求，因为一个chunk的读写，仅需要在最开始与master交互一次 来获取位置信息。这个所见对我们的负载极其有意义，因为应用程序总是顺序读写大型文件。即使是对于小文件随机读，client也可以恰当的为几TB的处理数据集维护chunk位置信息。第二，因为在一个大chunk上，client常常在这个chunk做多次操作，它可以通过长时间保持与chunkserver的持久TCP 连接来减少网络开销。第三，这减少了master存储的元数据大小。这使得我们可以在内存中保存元数据，这另外带来了其他收益（2.6.1讨论）。

另一方面，大chunk，即使使用惰性空间分配粗策略，也有它的优势。一个小文件仅由几个chunk组成，也有可能只有一个。如果许多客户端一起处理同一个小文件，存储这些chunk的chunkserver也许会成为热点。在实践中，这并没有成为一个主要问题，因为我们的应用基本都是顺序读取用于许多chunk的大型文件。

然而，当GFS第一次用在batch-queue系统中时，热点问题凸显出来：一个可执行文件写到GFS中，其只用一个chunk文件存储，然后同时在数百台机器启动。由于数百的并发请求，存储chunk文件的几个chunkserver就过载了。我们通过 高副本参数 和 交错启动batch-queue系统应用 解决了这个问题。一个可能的长期解决方案是，在这种情况下 允许client从其他client读取数据。

### 2.6 元数据

master存储了三个主要元数据：文件和chunk的名字空间、文件到chunk的映射、每个chunk副本的位置。所有元数据都保存在master的内存中。前两个数据（名字空间 和 file-to-chunk映射）也通过日志的形式写到master本地磁盘的操作日志（operation log）上，并且在远程机器上备份。使用日志的形式，让我们可以简单、可靠地更新master状态，并且在master crash时也不会有不一致的风险。master并不会持久化存储chunk位置信息。master会在启动的时候问询每个chunkserver的chunk信息，以及在有新chunkserver加入集群时。

#### 2.6.1 内存数据结构

因为元数据存储在内存中，master操作执行很快。此外，master周期性地后台扫描他的完整状态是很容易且高效的。周期性扫面用于实现chunk垃圾回收、chunkserver故障时再备份、为平衡负载和磁盘使用空间的chunkserver间的chunk迁移。4.3节和4.4节会讨论这些功能。

这种纯内存方法唯一担心的问题是chunk的数量，因为master的内存大小会因此限制整个系统的容量。在实践中，这并不严重。master为每个64MB的chunk维护小于64字节的元数据。因为大多数文件有多个chunk，所以大多数chunk都是满的，只有文件的最后一个chunk没有填满。类似的，文件的名字空间数据通常小于64字节，因为它使用前缀压缩来紧凑地存储文件名。

如果需要支持更大的文件系统，因 为简单性、可靠性、性能、灵活性的设计 的内存元数据，为master增加内存代价很小。

#### 2.6.2 块位置

master没有持久化的记录哪个chunk的副本在哪些chunkserver。在启动时轮训chunkserver来获取这些信息是很容易的事情。此后，master可以就可以自己保持更新这些数据了，因为它控制着所有chunk的摆放，并且以HearBeat消息的形式监控chunkserver的状态。

起初，我们尝试在master持久化保存chunk位置信息，我们最终这么决策是因为，master在启动时轮训chunkser然后周期性的HeatBeat的方案更简单。这消除了chunkserer加入离开集群、更名、故障、重启等保持master与chunkserver一致的问题，而且这些问题经常发生。

这个设计决策的另一个原因是，chunkserver拥有本地磁盘有什么chunk的最终话语权。尝试在master维护一致性视图是没有意义的，因为chunkserver的错误也许会导致chunk自发消失（例如，磁盘会坏或者不可用）或者一个操作员也许会重命名chunkserver。

#### 2.6.3 操作日志

操作日志包含里关键元数据变更的历史记录。他对GFS来说是重要的。它不仅是元数据的唯一持久记录，而且还充当定义了并发操作顺序的逻辑时间线。文件、chunk和他们的版本（见4.5节），都唯一地永久地被他们被创建的逻辑时间指定。

因为操作日志极其重要，我们必须可靠地存储它，并且直到元数据修改持久化前都不能相应给client。否则，即使块本身幸存下来，我们实际上也会丢失整个文件系统或最近的客户端操作。因此，我们在多台远程机器上做副本，并且只在相应的操作日志记录到本地和远程机器后 才给client回应。master会batch操作日志一次性来减少写磁盘和日志备份对系统整体吞吐量的影响。

master通过重演操作日志来恢复其文件系统状态。为了最小化启动时间，我们必须使日志最小化。每当操作日志增长超过了一定的大小，master会做checkpoint（存档），这样的化，他就可以从本地磁盘获取最近的存档然后只重演此后有限的日志记录。checkpoint是一个紧凑（compact）的B-tree形式，这样可以直接放到内存中，然后无需额外解析就可以用于名字空间查找。这极大地加速了恢复、提高了可用性。

因为创建一个checkpoint需要花点时间，master 的内部状态以这样一种方式构建，即可以在不延迟传入突变的情况下创建新的检查点。master切换到一个新的日志文件，然后一个独立线程创建新的checkpoint。这个checkpoint包含切到新的日志文件前的所有改动。为一个几百万文件的集群创建checkpoint可以在1min以内。完成时，他会被写到本地和远程的磁盘上。

恢复只需要最新完成的checkpoint和之后的日志文件。更老的checkpoint和日志文件都可以删除掉，但是我们为了防止更大的灾难会多存一些。创建checkpoint时的失败并不会引起正确性问题，因为恢复代码检测并跳过未完成的checkpoint。


### 2.7 一致性模型

GFS有一个可以很好支撑我们的高性能分布式应用的relaxed一致性模型，但可以相对简单和有效的实现。我们现在讨论GFS的保障（guarantees），以及他们对应用来说有什么意义。我们也强调GFS是怎么维护这些保障的，但是更多细节在论文后续部分讨论。

#### 2.7.1 GFS的保证

文件名字空间的操作（例如，创建）是原子的。他们仅由master处理：名字空间锁保证了原子性和正确性（4.1节）；master的操作日志定义了这些操作的全局顺序。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS-T1.png ':size=35%')

数据修改后的文件区域的状态，取决于 修改类型、是否成功、是否并发。表1总结了这些结果。如果所有client无论读的是哪个副本获取到的都是同样的数据，那么这个文件区域是一致的（consistene）。如果修改后文件区域是一致的且所有客户端都可以看到改动的内容，那么这个区域是确定的（defined）。如果修改不受并发writer影响地成功后，目标区域是确定的（也是一致的）：所有客户端都能看到修改了什么。并发的成功写让这块区域是不确定的（undifined）但是是一致的：所有client看到同样的数据，但它可能无法反应任何一个修改写入的内容。通常的，他有多个修改的混合片段组成。一个失败的修改另区域不一致（in-consistent），同样也不确定（undefined）：不通的client在不同时间可能会看到不同数据。我们下面描述，我们的应用如何分辨确定区域和不确定区域。应用程序不需要进一步区分不同类型的不确定区域。

数据修改可能是写入（writes）和记录追加（record appends）。写入的含义是写到应用特指的文件偏移的位置。记录追加，即使在并发修改的情况下也能在GFS选定的（3.3节）的偏移处原子的至少追加一次。相对的，记录追加仅仅是在client认为的当前文件结尾处写入数据。偏移量会返回给client，并标识包含该记录的确定区域的开头。另外，GFS可能会在中间插入空白（padding）或者重复记录。它们占据被认为不一致的区域，并且通常与用户数据量相形见绌。

在一些列成功的修改之后，被修改的文件区域保证是确定的，并且包含最后一个修改的写入数据。GFS通过在其所有副本上以相同顺序对块应用修改来实现这一点（3.1节），使用chunk版本号检测任何过期的副本（因为他在chunkserver关闭时错过了修改）（4.5节）。过期的副本永远不会参与修改，也不会给master返回client他的位置。他们是垃圾回收的最早时机。

因为client缓存chunk位置，他们可能在信息刷新前从过期副本读数据。这个窗口被缓存对象的过期时间限制，以及被下一次这个文件open的限制，这会从缓存中清除该文件的所有块信息。此外，我们大部分文件总是仅仅追加写，过期副本常常返回过期的chunk结尾，而不会返回过期的数据。当一个reader重试或者与master联系时，他会立即获取到chunk当前的位置信息。

成功修改很久之后，组件故障当然可能损坏或销毁数据。GFS通过master和chunkserver的正常握手来识别chunksever故障，并通过检验和检测数据损坏（5.2节）。一旦发生问题，数据会尽快从有效的副本重存（4.3节）。只有在几分钟内，GFS能反应过来前，一个chunk的所有副本都损坏（lost）这个chunk才会永久的丢失。即使在这种情况下，数据被确认是不可用，而不是未知损坏：应用会接收到明确错误，而不是损坏的数据。

#### 2.7.2 对应用的影响

GFS应用程序可以通过一些其他目的已经需要的简单技术来适应宽松的一致性模型：依靠追加而不是覆盖、做checkpoint、写自我验证/自我识别的记录。

实际上，我们所有的应用都是追加内容而不是覆盖写。在一个典型的场景中，一个writer从头到尾生成一个文件。写完所有数据后，原子的永久的重命名文件，或周期性的checkpoint已经成功写了多少。checkpoint也包括也许也包括应用级别的校验和。读者只验证和处理到最后一个检查点的文件区域，也就是确定是确定状态的位置。不管是一致性还是并发问题，这种方法都很好解决。追加写远比随机写高效，也更容易从应用故障恢复。checkpoint允许writer递增地重新启动，并防止reader处理从应用程序的角度来看 仍然不完整的 成功写入的文件数据。

在其他典型的使用场景，许多writer为了合并结果并发的追加内容到一个文件，或者作为一个producer-consumer队列。记录追加的追加最少一次（append-at-least-once）的语义保留了每个writer的输出。reader如下处理偶尔的填充数据和重复记录。每个记录都由writer准备了包含校验和的额外信息，这样他的合法性才能被校验。一个reader可以通过校验和识别并忽略额外的填充和记录片段。如果它不能容忍偶然的重复记录（例如，如果他们会触发不幂等的操作），可以使用记录中的唯一ID过滤掉它，应用也通常需要这个东西（例如，web文档）。在Google内，这些record I/O（不包括重复过滤）功能公共库代码的实行在应用中共享，并且适用于其他文件接口实现。这样，相同的记录序列，加上罕见的重复，总是能传递给记录reader。

## 3. 系统交互

我们设计系统来最大限度地减少master在所有操作中的参与。在这种背景下，我们现在描述client、master、chunkserver如何交互来实现数据修改、原子记录追加以及快照。

### 3.1 租约和修改顺序

修改是指一个改动chunk内容或者元数据的操作，例如write和append。每个修改都在chunk的所有副本进行。我们使用租约来保证副本见的一致性修改顺序。master给副本中的一个授予租约，这个被称之为primary。primary组织对这个chunk的一些列修改的顺序。所有副本在应用修改时遵循这个顺序。因此，要定义整体的修改顺序，首先要由master选择租约授权，在一个授权期间，primary安排这个顺序。

租约机制旨在最大限度地减少master的管理开销。一个租约最开始有60s的过期时间。然而，只要chunk还在修改期间，primary可以请求master并通常被应答延长租约。这个请求和应答在master和所有chunkserver的HeartBeat消息中捎带。master也许有时会在过期前撤销租约（例如，当master想要禁用正在重命名的文件的修改时）。即使master与primary丢失通信，当租约过期后，他可以安全的给其他副本授予一个新租约。

![](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/GFS-2.png ':size=35%')

在图2中，我们通过按照这些编号步骤的write的控制流程来说明此过程。

1. client想master问询哪个chunkserver拥有chunk的当前租约以及其他副本的位置。如果没有副本拥有租约，master选一个副本授予租约（未在图中描述）。
2. master回复primary的标识以及其他副本（secondary）的位置。client为了未来的修改缓存这些数据。他只有当primary变得不可达 或者 被primary告知不在用于租约时，才会再与master联系。
3. client给所有副本推送数据。client可以以任意顺序操作。每个chunksever在数据被使用或者过期前，把数据存储在内部LRU buffer cache。通过解藕数据流和控制流，我们可以通过基于网络拓扑调度昂贵的数据流来提高性能，而不用管哪个chunkserver是primary。我们在3.2节更深入讨论。
4. 一旦所有副本被告知接受完数据后，client发送一个write请求给primary。这个请求标识早前push给所有副本的数据。primary为它接收到的所有修改（可能从多个client）分配连续的序列号，这提供了必要的有序化。它以连续序列号的顺序本自己的本地状态执行修改。
5. primary向所有secondary副本转发write请求。每个secondary副本按照master安排的顺序执行修改。
6. 当secondary完成操作后，向primary回复消息。
7. primary回复client。任何副本的错误都会报告给client。在错误的情况下，write也许在primary或者secondary副本的任意子集成功。如果在primary失败，就不会分配序列号然后在secondary继续了。client请求被认为是失败的，修改的区域也处于不一致状态。我们的client对于这种失败修改会重试处理。他在从头重试前，会多次尝试在步骤3～7重试。

如果应用的write很大，或者跨越了chunk边界，GFS client代码把会它分成多个write操作。多个操作会按照上面描述的控制流执行，但是也许会 与其他client的操作交错执行 或 被其他client的操作覆盖。因此，共享的文件区域最终会包含来自不同client的片段，尽管副本将是相同的，因为单个操作在所有副本上以相同的顺序成功完成。这最终导致文件区域consistent但是却是undefined状态。

### 3.2 数据流

我们为了高效地使用网络，解藕了数据流和控制流。控制流从client到primary 然后到所有的secondary，数据以流水线方式沿着精心挑选的chunkserver链线性推送。我们的目标是尽量利用每台机器的网络带宽，避免网络瓶颈和高延迟，最大限度地减少推送所有数据的延迟。

为了充分利用每台机器的网络带宽，数据线性的按chunkserver链推送，而不是按其他拓扑结构（例如，树）的分布式地推送。因此，每台机器的全部出口带宽用于尽可能快地传输数据，而不是在多个接收者之间分配。

为了尽可能避免网络瓶颈和高延迟链接（例如，交换机间的链路），每台机器向他网络拓扑结构中“最近”的还未接收数据的机器推送。假设client要向chunkserver S1～S4发送数据。他向最近的机器发送数据，假如是S1。S1会向S2～S4中离他最近的机器转发数据，假如是S2。类似的，S2向S3、S4中离它最近的机器转发，以此类推。我们的网络拓扑结构是非常简单的，“距离”可以用IP地址准确的评估出来。

最后，我们通过流水线化（pipelining）TCP连接上的数据传输来最小化延迟。一旦chunkserver接受完一些数据，他就立即开始转发。流水线对我们特别有帮助，因为我们使用具有全双工链路的交换网络。立即转发数据并不会影响接受数据的速率。没有网络拥塞的理想情况下，传输B字节给R副本的预估时间是`B/T+RL`，T是网络吞储量，K是两台机器的间传输的延迟。我们的网络连接一般是100Mbps（T），L远小于1ms。因此，1MB理想情况下在80ms左右完成分发。

?> 1 B = 1 byte = 8 bit

### 3.3 原子记录追加

GFS提供了一个原子追加操作，record append。在传统写操作中，client指定数据写入的偏移量。同一文件同一区域的并发写是不可串行化的：这片区域最终可能会有多个client的数据片段。然而在record append中，client仅指定数据。GFS原子的（即，作为一个连续的字节序列）最少一次的把数据追加在文件内一个GFS决策的偏移位置，并返回client这个偏移位置。这与在Unix中以`O_APPEND`打开文件写数据类似，但没有多个writer并发写数据的竞争条件（race condition）问题。

record append在我们的分布式系统中广泛使用，许多不同机器的client并发的向同一个文件追加写。如果是传统模式的写操作，client就会需要复杂且代价昂贵的同步机制，例如分布式锁管理器。在我们工作中，这种文件充当multiple-producer/single-consumer队列 或者 容纳多个不同client的结果的合并。

record append是一种修改，控制流也大体如果3.1节所示，仅仅primary有一点额外的逻辑。client将数据推送到文件最后一个chunk的所有副本，然后将其请求发送到primary。primary检查，如果将数据追加到当前chunk后 会不会导致chunk超过最大大小（64MB）。如果会，primary就在当前chunk填满padding数据，告诉所有secondary也这样做，然后应答客户端说 这个操作将在下一个chunk重试。（record append被限制为最大块大小的四分之一，以将最坏情况的碎片保持在可接受的水平。）如果记录可以满足chunk的大小限制，这也是通常情况，primary把数据追加到他的副本上，然后告诉所有secondary写在primary指定的位置，最后返回成功给client。

如果一个记录在任意副本追加失败，client会重试。因此，同一块的副本可能包含不同的数据，可能包括同一记录的全部或部分副本。GFS不保证所有副本都是字节相同的。他只保证数据是至少一次的原子的单元。这个属性很容易从简单的观察中得出：报告成功的操作，数据必须以相同的偏移量写入某个chunk块的所有副本。而且，在此之后，所有副本一样长，因此 即使其他副本变成primary 未来任何记录都将被分配更高的偏移或新chunk中。依据我们的一致性保障，record append操作成功的区域，被写入的数据是确定的（因此，一致的），尽管这些区域是不一致的（因此，不确定的）。我们的应用可以依照2.7.2节所述处理不一致区域。

### 3.4 快照

快照操作是 在几乎一瞬间 对一个文件或者目录（“source”） 做备份，同时最大限度地减少正在进行的修改的任何中断。我们的用户使用它来快速创建庞大数据集的分支副本（并且经常是这些副本的副本，递归地），或者在提交实验性修改前 给当前状态做checkpoint，或者方便回滚。

像AFS一样，我们利用标准的copy-on-write技术来实现快照。当master收到快照请求，它首先撤销它即将创建快照的文件中块的任何未完成的租约。这确保了对这些块的任何后续写入都需要与 master 进行交互以找到租用持有者。这将使 master 有机会首先创建该块的新副本。

租约被撤销或到期后，master在磁盘记录这个操作（快照）日志。之后通过复制内存中的源文件或目录树的元数据 来执行这个日志记录。新创建的快照文件指向源文件相同的chunk。

在快照操作之后，client首次向chunk（C）写数据时，它向master请求查找当前的租约拥有者是谁。master会注意到chunk（C）的引用者不止一个，它推迟给client的应答，然后挑选一个新chunk（C'）。然后让每个拥有chunk（C）的chunkserver创建一个名字为C'的副本。在chunkserver的本地创建原件的副本，我们确保数据拷贝在本地发生，而不是通过网络（我们的磁盘比网络大概快3倍）。此后，任何chunk的请求处理都没什么差别：master给一个拥有chunk（C'）的chunkserver租约，然后应答给client，然后就可以正常的写chunk，也不用理解这时刚刚复制的chunk。

## 4. master操作

master处理所有的名字空间操作。另外，master管理系统间的chunk备份：副本安置决策、创建新chunk和副本、协调不同的系统维度的操作保证chunk已被完整备份、平衡所有chunkserver的负载、回收未使用的存储。我们现在讨论这些主题。

### 4.1 名字空间管理和锁

许多master的才做都需要花挺长时间：例如，快照操作需要撤销快照覆盖的所有chunk的chunkserver的租约。我们不想在此同时延后处理其他master操作。因此，我们允许多个操作同时进行，在命名空间的区域上使用锁以确保正确的序列化。

不像许多传统文件系统，GFS没有 列出该目录中所有文件的 每个目录的 数据结构。它也不支持同一文件或目录的别名（即 Unix 术语中的硬链接或符号链接）。GFS在逻辑上将其名字空间表示为将完整路径名映射到元数据的查找表。通过前缀压缩，可以在内存中有效地表示该表。名字空间树中的每个节点（文件或者目录的 绝对路径）都有一个关联的read-write锁。

每个master操作在执行前获取一系列锁。典型的，如果操作涉及`/d1/d2/../dn/leaf`，它会获取目录名为的`/d1`，`/d1/d2`……`/d1/d2/.../dn`的read-write锁，一个文件全路径`/d1/d2/.../dn/leaf`的读/写锁。注意取决于操作类型`leaf`可能是文件，也可能是目录。

我们现在说明这种锁定机制如何防止在`/home/user`被快照到`/save/user`时创建文件`/home/user/foo`。快照操作在`/home`和`/save`获取读锁，在`/home/user`和`/save/user`获取写锁。文件创建操作在`/home`和`/home/user`获取读锁，在`/home/user/foo`获取写锁。两个操作可以被合适的序列化，因为他们在尝试获取`/home/user`锁时有冲突。文件创建不用在父目录获取写锁，因为没有“目录“、或者innode样子的数据结构需要被保护。

这种锁定方案的一个很好的特性是它允许在同一目录中并发修改。例如，同一个目录的文件创建可以并发执行：每个操作都获取目录的读锁 和 文件名的写锁。目录名上的读锁足以防止目录被删除、重命名或快照。文件名上的写锁序列化尝试创建具有相同名称的文件两次。

因为名字空间可以有许多结点，读写锁对象被惰性分配并在不使用时删除。而且，以一致的顺序获取锁以防止死锁：按名字空间层级顺序，同级按照字典序。


### 4.2 副本安置（Placement）

GFS 集群高度分布在多个级别上。

### 4.3 创建、重备份、重平衡

### 4.4 垃圾回收

### 4.5 Stale复制检测

## 5. 容错和诊断

## 6. 指标

## 7. 经验

## 8. 相关工作

## 9. 总结
