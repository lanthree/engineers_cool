# Raft

> In Search of an Understandable Consensus Algorithm

## 摘要

> 原论文在[这里](https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf)

Raft是一种用于管理副本日志的共识算法（Consensus Algorithm）。它提供一个等同于(multi-)Paxos的结果，效率跟Paxos一样，但是组织结构不同于Paxos；这令Raft比Paxos更利于理解，对于建设实际系统提供了更好的基础。为了增加可理解性，Raft分离了共识的关键元素，例如，leader选举、日志副本和安全(safety)，他主张更强的一致性，来减少必须要考虑的状态的数量。一项用户研究的结果表明，对于学生来说，Raft比Paxos更容易学习。Raft还包括一个改变集群中成员关系的新机制，它使用重叠的多数来保证安全。

## 1. 介绍

共识算法允许一批机器像 能够经受其部分成员故障的一致的群体 一样工作。因此，在建设可靠的大型软件系统中，共识算法承担着关键角色。在过去十年中，Paxos算法统治着共识算法的讨论：大部分共识的实现都是基于Paxos或受其影响，Paxos也变成教导学生共识算法的重要工具。

不幸的是，尽管人们极力尝试让它平易近人，Paxos还是非常难懂。此外，为了支持实际系统，它的架构需要复杂的变化。结果，系统的构建者和学生都在与Paxos作斗争。

在亲自跟Paxos斗争之后，我们开始寻找一个新的 能为系统构建和教育提供更好基础 共识算法。我们的方法不同寻常，因为我们的首要目标是可理解性：我们是否可以为实际系统定义一个共识算法，并用一种明显比Paxos容易学习的方式描述它？另外，我们希望算法能够促进直觉的发展，这对于系统构建者来说是必不可少的。重要的不仅是算法是否有效，还在于它能明显地被理解为什么有效。

这项工作的成果是一个称为Raft的共识算法。在设计Raft时，我们应用特殊的技术来提高可理解性，包括分解（Raft分解leader选举、日志副本和安全）和减少状态空间（相对于Paxos，Raft减少了非确定性程度和服务器之间保持一致性的方法）。一项涉及2所大学43名学生的用户研究表明，Raft明显比Paxos容易理解：在学习完两个算法后，其中33名学习对Raft问题的回答比Paxos问题要好。

Raft在许多方面与现有的共识算法相似（最明显的是，Oki和Liskov的Viewstamped Replication），但是特有几个新颖的特性：

+ **强leader**（strong leader）：相对于其他共识算法，Raft使用一种更强形式的leader模式。例如，日志项（log entries）只能从leader向其他server流动。这减缓了备份日志的管理，也让Raft更容易理解。
+ **leader选举**（leader election）：Raft使用随机的timer来选举leader。这仅仅在任何共识算法都需要的heartbeat上增加一点点机制，但是简单快速地解决了冲突问题。
+ **成员关系变化**（membership changes）：Raft用于更改集群中的服务器集的机制使用了一种新的联合共识（*joint consensus*）方法，在这种方法中，两种不同配置的大部分服务 在转换期间 重叠（overlap）。这允许集群在配置更改期间继续正常运行。

我们相信，无论出于教育目的 还是最为实现基础，Raft都比Paxos更优秀。Raft比其他算法都简单，可理解性更强；描述完整，足以满足实际系统的需要；它有几个开源实现，被多家公司使用；其安全特性已得到正式规定和证明； 其效率可与其他算法相媲美。

论文的其余部分介绍了副本状态机（replicated state machine）问题（第2节），讨论Paxos的优缺点（第 3节），描述我们实现可理解性的一般方法（第4节），介绍Raft共识算法（第5-8节），评估Raft（第9节），并讨论相关工作（第10节）。

## 2. 副本状态机

共识算法通常出现在副本状态机（replicated state machine）的上下文中。在这个方法中，一组server的状态机计算相同状态的相同副本，能在其中一些server故障时继续服务。副本状态机用于在分布式系统中解决许多容错问题。举个例子，拥有单集群master的大型系统 例如 GFS、HDFS、RAMCloud，通常用一个分离的副本状态机来管理leader选举和存储配置信息，达到leader crash还能继续服务的目的。副本状态机的例子包括Chubby、ZooKeeper。

![图1：副本状态机架构。共识算法管理来自client的包含状态机命令的副本日志，让server产生同样的输出。](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/Raft_P1.png ':size=45%')

如图1所示，副本状态机通常用副本日志实现。每个server存储包含一系列命令的日志，他们的状态机按日志顺序执行。每个日志包含同样顺序的同样命令，所以每个状态机按同样的命令处理同样的命令。因此，这些状态机是确定的，每个计算同样的状态，输出同样的序列。

保持副本日志的一致性就是共识算法的工作。server上的共识模块从多个client接受不了命令，并把命令添加到它的日志中。它与其他server上的共识模块交流，来确保，即使某些server故障，每个日志最终包含相同顺序的同样命令。一旦命令被合适的备份，每个server的状态机按日志顺序处理这些命令，输出被返回给那些client。最终结果，server看上去形成一个单一的、高可用的状态机。

实际系统的共识算法通常具有以下特性：

+ 算法确保在所有非拜占庭（non-Byzantine）条件下的安全性（safety，从不返回不正确的结果）,包括网络延迟、分区（partitions）、丢包、重入和重排序（reordering）等。
    + [拜占庭问题](https://zh.wikipedia.org/wiki/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98)。简单解释是：有间谍（叛徒），跟不同/相同对象返回的决策结果不一致。
+ 只要任意大多数server可执行且能其他server/client通信，算法就是功能完好的（可用的）。因此，通常5台server的集群，可以容忍任意2台server故障。server假设的故障是停止工作；他们也许稍后会从稳定存储中恢复状态并重新加入集群。
+ 算法不能依赖时序（timing）来确保日志的一致性：最坏情况下，时钟错误和极端的消息延迟能导致可用性问题。
+ 通常情况下，只要集群的大多数成员响应了一轮RPC，命令就可以完成；少数速度较慢的服务器需要不影响整个系统的性能。

## 3. Paxos的问题是什么？

在过去十年中，Leslit Lamport的Paxos协议几乎已经成为共识的同义词：它是在课堂中最常教授的协议，大多数共识的实现都将其作为起点。Paxos首先定义了一种 能够就单个决定达成一致 的协议，例如单个日志项备份。我们称这个子集为*single-decree Paxos*。Paxos然后组合该协议的多个实例，以促进一系列决策，如日志(multi-Paxos)。Paxos确保安全性和活性，并且支持集群成员的更改。其正确性已被证明，在通常情况下是有效的。

不幸的是，Paxos有两大弊端。首先，Paxos格外的难以理解。众所周知，完整的解释是模糊的（opaque）；很少有人能够理解它，还需要很大的努力。所以，对于 用简单术语解释Paxos 有很多尝试。这些解释关注于*single-decree Paxos*，但是他们仍然很有挑战性。在一项NSDI 2012参与者的非正式调查中，我们发现，即使是经验丰富的研究人员，也很少有人对Paxos感到舒适。我们亲自跟Paxos作斗争；在 阅读完几个简化解释 和 设计我们自己的替代的协议 前，我们都不能理解完整的协议，而这个过程耗费了一年时间。

我们假设Paxos的不模糊性（opaqueness）来自于它选择single-decree subset作为其基础。Single-decree Paxos是密集而微妙的：它分为没有简单直接解释的两段，而且不能被独立理解。因此，很难产生对于为什么single-decree有效的直观理解。multi-Paxos的组合规则显著增加了复杂度和巧妙性。我们认为，就多项决定达成协商一致意见的总体问题（也就是说，一个日志文件而不是一个单一的条目）可以用其他更直接和明显的方式来分解。

Paxos的第二个问题是，它没有为实际实现提供良好的基础。其中一个原因是，对于multi-Paxos并没有广泛认可的算法。Lamport的描述大多是关于single-decree Paxos；他大致描绘了实现multi-Paxos的可能方法，但是丢失很多细节。已经有几次尝试充实和优化Paxos，但是它们彼此不同，也与Lamport的描绘不同。例如Chubby的系统已经实现了Paxos样子的算法，但在大多数情况下，他们的细节并没有公开。

此外，Paxos结构对于构建实用系统来说是一个糟糕的架构；这是single-decree分解的另一个后果。例如，独立选择日志条目的集合，然后将它们合并到一个顺序日志中几乎没有什么好处；这仅仅增加了复杂性。围绕日志设计一个系统更简单、更有效，其中新条目以受约束的顺序依次附加。另一个问题是，Paxos 在其核心使用对称点对点方法（尽管它最终建议了一种弱领导形式作为性能优化）。在一个只有单个决策问题的简单世界这是有意义的，但是很少有系统使用这种方法。如果必须要做一系列决策，在最开始是选举leader 然后由leader协调决策是 更简单、更快速的。

所以，实际系统与Paxos几乎没有相似之处。每个以Paxos为开始的实现，发现实现它的困难点，然后就会发开一个明显不同的架构。这是费时且容易出错的，而且理解Paxos的困难加剧了问题。Paxos公式（formulation）也许是一个证明它的理论正确性的公式，但实际的实现与Paxos是如此不同，以至于证明几乎没有价值。典型的，下面是来自Chubby实现者的评论：

> Paxos算法的描述 与 真实世界系统的需求 有显著的隔阂......最终的系统会基于一个没有被证明的协议。

因为这些问题，我们认为Paxos没有为系统构建和教育提供很好的基础。在 大型软件系统中共识算法的重要性 背景下，我们决定尝试看我们能否设计一个替代性的 有比Paxos更好特性的 共识算法。Raft就是这个尝试的结果。

## 4. 为了可理解性的设计

设计Raft的目标有多个：它必须为系统构建提供完整的可事件的基础，这样就可以显著减少开发者需要的设计工作；它必须在所有条件下都是安全的，并在典型的操作条件下可用；它必须对普通操作有效。然而我们最重要的目标——也是最大的挑战——是可理解性。它必须能够让大量的用户（audience）轻松地理解算法。此外，它必须有可能开发关于算法的直觉，以便系统构建者可以进行在现实世界实现中不可避免的扩展。

在Raft的设计中，我们需要在许多可选择的方法中进行选择。在这些情况下，我们基于可理解性来评估备选方案：每一种选择解释起来有多难（例如，它的状态空间有多复杂，它有微妙的暗示吗？），以及 对于读者来说，完全理解这种方法及其含义有多容易。

我们认识到在这种分析中存在高度的主观性；尽管如此，我们使用了两种普遍适用的技术。第一种技术是众所周知的问题分解方法：只要有可能，我们就把问题分成可以相对独立地解决、解释和理解的独立部分。例如，在Raft中，我们分离了领导人选举、日志副本、安全以及成员关系变更。

我们的第二个方式是，通过减少考虑的状态的数量来简化状态空间，使系统更加清晰易懂（coherent），尽可能消除不确定性。具体来说，日志不允许有hole，Raft限制了日志相互之间不一致的方式。尽管在大多数情况下，我们试图消除不确定性，在某些情况下，不确定性实际上提高了可理解性。特别地，随机方法引入了不确定性，但它们倾向于通过以类似的方式处理所有可能的选择（选择任意;没关系）。我们用随机化的方法简化了Raft leader的选举算法。

## 5. 共识算法Raft

Raft是用于管理第2节中描述的日志副本的算法。图2以表格的总结了算法，以供参考，图3列举了算法的关键属性；这些图的元素会在接下来的章节展开讨论。

![图2：共识算法Raft的总结（包括成员关系变更和日志压缩）。左上角框中的server行为被描述为一组独立且重复触发的规则。章节号（例如§5.2）表示这一特性将会在哪一节讨论。](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/Raft_P2.png)

![图3：Raft保证在任何时间每一个属性都是正确的。章节号表示这一属性在哪一节讨论。](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/Raft_P3.png ':size=45%')

Raft这么实现共识：首先选举出一个leader，然后给这个leader管理日志副本的完整责任。leader从client接收日志项，备份到其他server，并告诉其他server什么时候可以安全的应用日志项到它的状态机。有一个leader的设定，简化了日志副本的管理。例如，leader可以在不需要跟其他server交流的情况下独立决策新的日志项应该放在日志的哪个地方，而且数据也以一种简单的方式从leader流到其他服务器。leader可以故障，可一个与其他server断开连接，在这种情况下，会选举一个新leader。

在leader方法中，Raft分解共识问题为三个相对独立的子问题，这些子问题将会在下面的子章节中讨论：

+ **leader选举**（leader election）：当现leader故障时，必须选举一个新leader（5.2节）。
+ **日志备份**（log replication）：leader必须接收client的日志项，并把他们备份到全部集群server，促使其他（server的）日志跟他的一样（5.3节）。
+ **安全**（safety）：Raft关键安全属性是图3中的状态机安全属性（State Machine Safety Property）：如果任意server已经对他的状态机应用了特定的日志项，那么没有其他server会在同样的日志index使用不同的命令（不同的日志项）。5.4节描述了Raft如何确保这一属性；解决方案包括一个在 5.2节描述的选举机制 的额外约束。

在阐述完共识算法后，这一节讨论可用性的问题，和定时（timing）在系统中的作用。

### 5.1 Raft基础

一个Raft集群包含多台server：典型地是5台，这样系统能容忍任意两台故障。在任意给定的时间，每个server都处理下面三个状态中的一个：leader、follower、candidate。在正常的操作中，只会有一个leader，其他server都是follower。follower都是被动的：他们不自己执行请求，只会简单的响应leader和candidate的请求。leader处理所有client的请求（如果client联系到follower，follower会重定向请求到leader）。第三个状态，candidate（在5.2节描述），用于选举一个新的leader。图4展示了状态和他们的转换；下面会讨论这些转换。

![图4：server状态。follower只响应其他server的请求。如果一个follower收不到任何请求，他就会变成candidate，开始选举。candidate如果接收到集群大多数server的投票，他就会成为新的leader。leader通常会一直运作到其故障。](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/Raft_P4.png ':size=45%')

![图5：时间被划分为term，每个term以选举开始。选举成功后，单leader管理集群，直到term结束。一些选举失败，在这些情况下term会以无leader结束。在不同的服务器上，可以在不同的时间观察term之间的转换。](https://engineers-cool-1251518258.cos.ap-chengdu.myqcloud.com/Raft_P5.png ':size=45%')

如图5所示，Raft把时间划分为任意长度的term。term以连续的整型数字编号。每个term以选举开始，这时一个或多个candidate尝试成为leader（5.2节描述）。如果一个candidate赢得选举，它就会在term剩下的时间以leader的形式服务。在某些情况下，一次选举会造成投票分裂。在这种term中，会没有leader；很会会开始新term，开始新的选举。Raft确保在一个给定的term中最多只有一个leader。

不同的server会在不同的时间观察到term之间的转变，在某些情况下，一个server也许不会观察到一次选举，甚至整个term周期。term就像一个Raft的逻辑时钟，让server能检测过时的信息，如陈旧的leader（stale leader）。每个server存储一个当前term编号，这个编号会随时间单调递增。当前term在server通信时交换；如果一个server的当前term小于其他server，那么他会更新它的当前term为更大的值。如果一个candidate或者leader发现它的term过期了，它会立即退回为follow状态。如果一个server收到了一个过期term编号的请求，它会拒绝这个请求。

Raft的server交流使用RPC，基础的共识算法只需要两种类型的RPC。RequestVote RPC，由candidate在选举期间发出（5.2节），AppendEntries RPC由leader在备份日志项和提供心跳时发出（5.3节）。第7节为了在server间传送快照引入了第三种RPC。如果server没有一定的时间内接收到响应，就会重试RPC，另外server会并行使用RPC来最优化性能。

### 5.2 leader选举

Raft使用心跳机制来触发leader选举。当server启动时，先成为follower。只要server能从leader或者candidate接收到有效RPC，他就保持为follower状态。leader为了保持其权利会周期性发送心跳（不携带日志项的AppendEntries RPC）给所有foloower。如果一个follower在election timeout时间内都没有收到任何请求，他就会假设没有活着的leader，然后开始一轮选举来选择一个新的leader。

要开始一轮选举，一个follower增加它当前的term编号，状态变更为candidate。
